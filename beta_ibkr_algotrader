# =====================
# CORE IMPORTS
# =====================
from ib_insync import *
import pandas as pd
import numpy as np
import mplfinance as mpf
from datetime import datetime, timedelta, time
import asyncio
import nest_asyncio
import logging
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from time import sleep
import calplot
from matplotlib.colors import ListedColormap
from collections.abc import MutableMapping  # FIX: Added correct abstract base class import
import pytz

# =====================
# LINKED LIST DICTIONARY IMPLEMENTATION
# =====================
class LLNode:
    __slots__ = ('key', 'value', 'next')
    def __init__(self, key, value, next=None):
        self.key = key
        self.value = value
        self.next = next

class LLDict(MutableMapping):  # FIX: Replaced DictAbstract with MutableMapping
    def __init__(self):
        self.head = None
        self._size = 0

    def __setitem__(self, key, value):
        current = self.head
        while current is not None:
            if current.key == key:
                current.value = value
                return
            current = current.next
        self.head = LLNode(key, value, self.head)
        self._size += 1

    def __getitem__(self, key):
        current = self.head
        while current is not None:
            if current.key == key:
                return current.value
            current = current.next
        raise KeyError(key)

    def __delitem__(self, key):
        prev = None
        current = self.head
        while current is not None:
            if current.key == key:
                if prev:
                    prev.next = current.next
                else:
                    self.head = current.next
                self._size -= 1
                return
            prev = current
            current = current.next
        raise KeyError(key)

    def __iter__(self):
        current = self.head
        while current is not None:
            yield current.key
            current = current.next

    def __len__(self):
        return self._size

# =====================
# GLOBAL CONFIGURATION
# =====================
symbol = "CVNA"
initial_balance = 10000

# ====================
# CALENDAR VISUALISATION
# ====================
def plot_pnl_calendar(engine, start_date, end_date):
    """Creates calendar heatmap showing daily PNL performance"""
    # Convert dates to UTC timezone
    start_date = start_date.astimezone(pytz.UTC).replace(tzinfo=None)
    end_date = end_date.astimezone(pytz.UTC).replace(tzinfo=None)
    
    # Create UTC-based date range
    date_range = pd.date_range(start=start_date, end=end_date, freq='D', tz='UTC')
    
    # Convert engine dates to UTC for matching
    pnl_data = {
        pd.Timestamp(k).tz_localize('UTC').date(): v 
        for k, v in engine.daily_pnl.items()
    }
    
    pnl_series = pd.Series(
        [pnl_data.get(d.date(), 0.0) for d in date_range],
        index=date_range.tz_convert(None)  # Remove tz for calplot compatibility
    )
    
    # Rest of the function remains the same
    pnl_sign = pnl_series.apply(
        lambda x: 1 if x > 0 else (-1 if x < 0 else np.nan)
    )
    
    cmap = ListedColormap(['#ff4444', '#44ff44'])
    
    plt.figure(figsize=(16, 10))
    calplot.calplot(
        pnl_sign,
        cmap=cmap,
        fillcolor='whitesmoke',
        how='sum',
        edgecolor='white',
        linewidth=0.5,
        daylabels='MTWTFSS',
        dayticks=[0, 2, 4, 6],
        monthticks=True,
        dropzero=True
    )
    plt.title(f'Daily Trading Performance: {start_date.year}', pad=20)
    plt.show()

# =====================
# BACKTESTING ENGINE
# =====================
class BacktestEngine:
    def __init__(self):
        self.balance = initial_balance
        self.positions = LLDict()
        self.trade_history = []
        self.current_position = None
        self.fee_per_share = 0.0035
        self.slippage = 0.0002
        self.trade_markers = []
        self.daily_pnl = {}
        self.entry_conditions = {
            'ema_order': False,
            'rsi_threshold': False,
            'position_status': False
        }

    def apply_slippage(self, price, is_buy):
        """
        Applies slippage to the execution price
        """
        slippage_factor = 1 + (self.slippage if is_buy else -self.slippage)
        return price * slippage_factor

    def calculate_fees(self, shares):
        """
        Calculates trading fees based on number of shares
        """
        return abs(shares * self.fee_per_share)

    def execute_trade(self, ticker, price, shares, action, timestamp):
        executed_price = self.apply_slippage(price, action=='BUY')
        fees = self.calculate_fees(shares)
        position_value = abs(shares * executed_price)
        
        trade = {
            'timestamp': timestamp,
            'action': action,
            'price': executed_price,
            'shares': shares,
            'position_value': position_value,
            'fees': fees,
            'new_balance': self.balance
        }

        trade_date = pd.Timestamp(timestamp).date()
        if trade_date not in self.daily_pnl:
            self.daily_pnl[trade_date] = 0.0

        if action == 'SELL' and self.current_position:
            pnl = (executed_price - self.current_position['entry_price']) * shares
            self.daily_pnl[trade_date] += pnl
            trade['pnl'] = pnl
            trade['pnl_pct'] = (pnl / self.current_position['entry_price']) * 100
        
            self.balance += pnl - fees
            self.current_position = None # Reset current position on sell

        elif action == 'BUY':
            self.balance -= position_value + fees
            self.current_position = {
                'entry_price': executed_price,
                'shares': shares,
                'timestamp': timestamp
            }
            self.trade_markers.append({
                'date': timestamp,
                'price': executed_price,
                'marker': '↑' if action == 'BUY' else '↓'
            })

        self.trade_history.append(trade)  # Append trade to history

        return trade

# Initialize engine globally
engine = BacktestEngine()

# =====================
# STRATEGY CONDITIONS
# =====================
def check_entry_conditions(df, index):
    try:
        row = df.loc[index]
        # Verify market hours (9:30 AM to 4:00 PM ET)
        if not (time(9,30) <= index.time() <= time(16,0)):
            return False
            
        conditions = {
            'ema_order': (row['1_EMA'] > row['3_EMA'] > row['8_EMA'] > row['VWAP']),
            'rsi_threshold': row['RSI'] < 70,
            'position_status': not engine.current_position,
            'valid_vwap': pd.notnull(row['VWAP'])
        }
        return all(conditions.values())
    except KeyError as e:
        logger.error(f"Missing indicator column: {str(e)}")
        return False

def check_exit_conditions(df, index):
    if not engine.current_position:
        return False
        
    row = df.loc[index]
    exit_conditions = [
        row['1_EMA'] < row['3_EMA'],
        row['3_EMA'] < row['8_EMA'],
        row['RSI'] > 70,
        row['close'] < engine.current_position['entry_price'] * 0.98
    ]
    return any(exit_conditions)

# =====================
# ENHANCED LOGGING
# =====================
class TradeFormatter(logging.Formatter):
    GREEN = '\033[92m'
    RED = '\033[91m'
    RESET = '\033[0m'
    
    def format(self, record):
        if 'BUY' in record.msg:
            record.msg = f"{self.GREEN}{record.msg}{self.RESET}"
        elif 'SELL' in record.msg:
            record.msg = f"{self.RED}{record.msg}{self.RESET}"
        return super().format(record)

# Configure root logger directly
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)

# Clear existing handlers
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)

# Create and configure handlers
handler = logging.StreamHandler()
handler.setFormatter(TradeFormatter('%(asctime)s | %(message)s'))
root_logger.addHandler(handler)

file_handler = logging.FileHandler('strategy_execution.log')
file_handler.setFormatter(logging.Formatter('%(asctime)s | %(message)s'))
root_logger.addHandler(file_handler)

logger = logging.getLogger(__name__)

def log_trade(trade, symbol, retries=3):
    """
    Enhanced trade logging with robust timestamp handling
    """
    # Validate trade structure first
    required_keys = {'timestamp', 'price', 'position_value', 'shares', 'action'}
    if missing := required_keys - trade.keys():
        logger.error(f"Missing trade keys: {missing}")
        return

    # Parse timestamp with proper timezone handling
    try:
        trade_time = pd.to_datetime(trade['timestamp'], utc=True)
        trade_time = trade_time.tz_convert('America/New_York')
    except Exception as e:
        logger.error(f"Invalid timestamp {trade['timestamp']}: {str(e)}")
        return

    # Build log messages
    log_parts = [
        f"{trade['action']} | {symbol}",
        f"Time: {trade_time.strftime('%Y-%m-%d %H:%M:%S%z')}",
        f"Price: ${trade['price']:.2f}",
        f"Size: ${trade['position_value']:.2f}",
        f"Shares: {trade['shares']:.2f}"
    ]
    
    if trade['action'] == 'SELL':
        log_parts.extend([
            f"PnL: ${trade['pnl']:.2f}",
            f"Return: {trade['pnl_pct']:.2f}%"
        ])
    
    logger.info(" | ".join(log_parts))

# =====================
# DATA MANAGEMENT
# =====================
nest_asyncio.apply()

def get_historical_data(ib, symbol, exchange='SMART', currency='USD', backtest=False):
    contract = Stock(symbol, exchange, currency)
    
    if not backtest:
        bars = ib.reqHistoricalData(
            contract,
            endDateTime='',
            durationStr='1 D',
            barSizeSetting='5 mins',
            whatToShow='TRADES',
            useRTH=True,
            formatDate=1,
            keepUpToDate=False
        )
        df = util.df(bars)
        
        # Process data
        df = df.dropna(subset=['volume'])
        df = df[df['volume'] > 0]
        df['date'] = pd.to_datetime(df['date']).dt.tz_convert('America/New_York')
        
        # Calculate indicators
        for window in [1, 3, 8]:
            df[f'{window}_EMA'] = df['close'].ewm(span=window, adjust=False).mean()
            
        # Calculate RSI
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0.0)
        loss = -delta.where(delta < 0, 0.0)
        avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()
        avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()
        df['RSI'] = 100 - (100 / (1 + (avg_gain / avg_loss)))
        
        # Calculate VWAP
        df['trading_date'] = df['date'].dt.date
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        df['tp_vol'] = df['volume'] * typical_price
        df['cumulative_tp_vol'] = df.groupby('trading_date')['tp_vol'].cumsum()
        df['cumulative_vol'] = df.groupby('trading_date')['volume'].cumsum()
        df['VWAP'] = df['cumulative_tp_vol'] / df['cumulative_vol']
        
        # Cleanup temporary columns
        df = df.drop(['tp_vol', 'cumulative_tp_vol', 'cumulative_vol', 'trading_date'], axis=1)
        df.set_index('date', inplace=True)
        return df, df, df  # Return same dataframe three times for compatibility
    
    if backtest:
        # Modified duration and bar size settings
        bars = ib.reqHistoricalData(
            contract,
            endDateTime='',  # Empty string means current time
            durationStr='30 D',
            barSizeSetting='1 hour',
            whatToShow='TRADES',
            useRTH=True,
            formatDate=1,
            keepUpToDate=False  # Ensure we get historical data only
        )
        
        if not bars:
            raise ValueError("No data received from IB")
            
        df = util.df(bars)
        
        # Ensure we have data before calculating indicators
        if len(df) == 0:
            raise ValueError("Empty dataframe received from IB")
            
        # Calculate technical indicators
        for window in [1, 3, 8]:
            df[f'{window}_EMA'] = df['close'].ewm(span=window, adjust=False).mean()
        
        # Calculate VWAP properly for intraday data
        df['vwap_numerator'] = df['volume'] * (df['high'] + df['low'] + df['close']) / 3
        df['vwap_denominator'] = df['volume']
        
        # Group by date to reset VWAP calculations daily
        df['date'] = pd.to_datetime(df['date'])
        df['trading_date'] = df['date'].dt.date
        
        # Calculate VWAP for each trading day
        df['VWAP'] = (df.groupby('trading_date')['vwap_numerator'].cumsum() / 
                      df.groupby('trading_date')['vwap_denominator'].cumsum())
        
        # Clean up temporary columns
        df = df.drop(['vwap_numerator', 'vwap_denominator', 'trading_date'], axis=1)
        
        # Calculate RSI
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0).ewm(alpha=1/14, adjust=False).mean()
        loss = (-delta.where(delta < 0, 0)).ewm(alpha=1/14, adjust=False).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))
        
        df.set_index('date', inplace=True)
        return df

    return get_historical_data(ib, symbol, exchange, currency, False)

# =====================
# TECHNICAL ANALYSIS
# ===================== 

def find_support_resistance(df_5m, df_15m, df_30m, window=20, min_touches=3, price_threshold=0.15):
    levels = {'support': [], 'resistance': []}
    touches = {}
    
    # Calculate swing highs and lows for all timeframes
    df_5m['swing_high'] = df_5m['high'].rolling(window=5, center=True).max()
    df_5m['swing_low'] = df_5m['low'].rolling(window=5, center=True).min()
    
    df_15m['swing_high'] = df_15m['high'].rolling(window=3, center=True).max()
    df_15m['swing_low'] = df_15m['low'].rolling(window=3, center=True).min()
    
    df_30m['swing_high'] = df_30m['high'].rolling(window=2, center=True).max()
    df_30m['swing_low'] = df_30m['low'].rolling(window=2, center=True).min()
    
    # Get potential levels from higher timeframes
    higher_tf_levels = set()
    
    # Add 15m levels
    for _, row in df_15m.iterrows():
        higher_tf_levels.add(row['swing_high'])
        higher_tf_levels.add(row['swing_low'])
    
    # Add 30m levels
    for _, row in df_30m.iterrows():
        higher_tf_levels.add(row['swing_high'])
        higher_tf_levels.add(row['swing_low'])
    
    # Process 5m data with confluence from higher timeframes
    price_history = []
    for i in range(len(df_5m)):
        current_price = df_5m.iloc[i]
        price_history.append({
            'high': current_price['high'],
            'low': current_price['low'],
            'close': current_price['close'],
            'swing_high': current_price['swing_high'],
            'swing_low': current_price['swing_low']
        })
        
        if len(price_history) >= window:
            window_prices = price_history[-window:]
            
            if i % 3 == 0:
                current_high = window_prices[-1]['swing_high']
                current_low = window_prices[-1]['swing_low']
                
                # Check for confluence with higher timeframe levels
                has_higher_tf_confluence_high = any(abs(level - current_high)/current_high < price_threshold 
                                                  for level in higher_tf_levels)
                has_higher_tf_confluence_low = any(abs(level - current_low)/current_low < price_threshold 
                                                 for level in higher_tf_levels)
                
                support_count = 0
                resistance_count = 0
                
                for k in range(len(window_prices) - 1):
                    # Support level logic with confluence
                    if abs(window_prices[k]['low'] - current_low) / current_low < price_threshold:
                        if (window_prices[k+1]['close'] > current_low and 
                            window_prices[k+1]['low'] > window_prices[k]['low'] and
                            has_higher_tf_confluence_low):
                            support_count += 1
                    
                    # Resistance level logic with confluence
                    if abs(window_prices[k]['high'] - current_high) / current_high < price_threshold:
                        if (window_prices[k+1]['close'] < current_high and 
                            window_prices[k+1]['high'] < window_prices[k]['high'] and
                            has_higher_tf_confluence_high):
                            resistance_count += 1
                
                # Add levels with strong confirmation and higher timeframe confluence
                if support_count >= min_touches:
                    levels['support'].append(current_low)
                    touches[current_low] = support_count
                
                if resistance_count >= min_touches:
                    levels['resistance'].append(current_high)
                    touches[current_high] = resistance_count
    
    # Improved level filtering
    for level_type in ['support', 'resistance']:
        levels[level_type] = sorted(set(levels[level_type]))
        filtered = []
        
        # Group nearby levels
        current_group = []
        for price in levels[level_type]:
            if not current_group or abs(price - current_group[-1])/current_group[-1] <= 0.005:
                current_group.append(price)
            else:
                # Take the most touched level from the group
                best_level = max(current_group, key=lambda x: touches[x])
                filtered.append(best_level)
                current_group = [price]
        
        if current_group:
            best_level = max(current_group, key=lambda x: touches[x])
            filtered.append(best_level)
        
        levels[level_type] = filtered
    
    return levels, touches

def get_color_intensity(touch_count, min_touches, max_touches):
    intensity = 0.5 + 0.5 * ((touch_count - min_touches) / (max_touches - min_touches))
    return min(max(intensity, 0.5), 1.0)

def find_trendlines(df_5m, min_touches=3):
    trendlines = {'bullish': [], 'bearish': []}
    
    # Create lists to store potential trend points
    bullish_sequences = []
    bearish_sequences = []
    current_bullish = []
    current_bearish = []
    
    # Helper function to check if a candle is green/red
    def is_green_candle(row):
        return row['close'] > row['open']
    
    def is_red_candle(row):
        return row['close'] < row['open']
    
    # Iterate through the dataframe
    for i in range(len(df_5m)):
        current_row = df_5m.iloc[i]
        
        # Check for bullish sequence
        if is_green_candle(current_row):
            if not current_bullish:
                current_bullish = [(df_5m.index[i], current_row['close'])]
            else:
                if current_row['close'] > current_bullish[-1][1]:
                    current_bullish.append((df_5m.index[i], current_row['close']))
                else:
                    if len(current_bullish) >= min_touches:
                        bullish_sequences.append(current_bullish)
                    current_bullish = [(df_5m.index[i], current_row['close'])]
        
        # Check for bearish sequence
        if is_red_candle(current_row):
            if not current_bearish:
                current_bearish = [(df_5m.index[i], current_row['close'])]
            else:
                if current_row['close'] < current_bearish[-1][1]:
                    current_bearish.append((df_5m.index[i], current_row['close']))
                else:
                    if len(current_bearish) >= min_touches:
                        bearish_sequences.append(current_bearish)
                    current_bearish = [(df_5m.index[i], current_row['close'])]
    
    # Create trend lines from sequences
    for sequence in bullish_sequences:
        if len(sequence) >= min_touches:
            trend_line = pd.Series(index=df_5m.index, dtype=float)
            start_date, start_price = sequence[0]
            end_date, end_price = sequence[-1]
            
            # Calculate line gradient
            time_delta = (end_date - start_date).total_seconds()
            price_delta = end_price - start_price
            gradient = price_delta / time_delta
            
            # Create trend line points
            for date in df_5m.index:
                if start_date <= date <= end_date:
                    seconds_from_start = (date - start_date).total_seconds()
                    trend_line[date] = start_price + (gradient * seconds_from_start)
            
            trendlines['bullish'].append(trend_line)
    
    for sequence in bearish_sequences:
        if len(sequence) >= min_touches:
            trend_line = pd.Series(index=df_5m.index, dtype=float)
            start_date, start_price = sequence[0]
            end_date, end_price = sequence[-1]
            
            # Calculate line gradient
            time_delta = (end_date - start_date).total_seconds()
            price_delta = end_price - start_price
            gradient = price_delta / time_delta
            
            # Create trend line points
            for date in df_5m.index:
                if start_date <= date <= end_date:
                    seconds_from_start = (date - start_date).total_seconds()
                    trend_line[date] = start_price + (gradient * seconds_from_start)
            
            trendlines['bearish'].append(trend_line)
    
    return trendlines

# =====================
# VISUALIZATION ENGINE
# =====================
def plot_candlestick(df, backtest=False):
    addplots = []
    
    # Add technical indicators using EMA dictionary mapping
    ema_colors = {1: 'blue', 3: 'orange',8: 'red'}
    for ema in [1, 3, 8]:
        addplots.append(mpf.make_addplot(
            df[f'{ema}_EMA'],
            color=ema_colors[ema],
            width=1.5,
            alpha=0.8
        ))
    
    addplots.append(mpf.make_addplot(
        df['VWAP'],
        color='purple',
        width=1.5,
        alpha=0.8
    ))
    
    # Handle trade markers for backtest mode
    if backtest and engine.trade_markers:
        markers_df = pd.DataFrame(engine.trade_markers)
        marker_series = pd.Series(np.nan, index=df.index)
        
        # Fill marker series with prices at correct dates
        for _, row in markers_df.iterrows():
            marker_series.at[row['date']] = row['price']
        
        # Plot buy markers
        buy_mask = markers_df['marker'] == '↑'
        if any(buy_mask):
            buy_series = marker_series.copy()
            buy_series[~marker_series.index.isin(markers_df[buy_mask]['date'])] = np.nan
            addplots.append(mpf.make_addplot(
                buy_series,
                type='scatter',
                markersize=80,
                marker='$\\uparrow$',  # Thin up arrow
                color='#00FF00'  # Bright green
            ))
        
        # Plot sell markers
        sell_mask = markers_df['marker'] == '↓'
        if any(sell_mask):
            sell_series = marker_series.copy()
            sell_series[~marker_series.index.isin(markers_df[sell_mask]['date'])] = np.nan
            addplots.append(mpf.make_addplot(
                sell_series,
                type='scatter',
                markersize=80,
                marker='$\\downarrow$',  # Thin down arrow
                color='#FF0000'  # Bright red
            ))
    
    # Plot configuration
    mc = mpf.make_marketcolors(
        up='green',
        down='red',
        edge='inherit',
        wick='inherit',
        volume={'up':'green', 'down':'red'}
    )
    
    s = mpf.make_mpf_style(
        marketcolors=mc,
        gridstyle='dotted',
        base_mpf_style='charles'
    )
    
    # Create the plot
    fig, axlist = mpf.plot(
        df,
        type='candle',
        style=s,
        volume=True,
        addplot=addplots,
        figsize=(12, 8),
        panel_ratios=(6, 2),
        returnfig=True
    )
    
    # Add legend
    ax = axlist[0]
    ax.legend(
        handles=[
            Patch(facecolor="blue", label="1 EMA"),
            Patch(facecolor="orange", label="3 EMA"),
            Patch(facecolor="red", label="8 EMA"),
            Patch(facecolor="purple", label="VWAP")
        ],
        loc='upper center',
        bbox_to_anchor=(0.5, 1.1),
        ncol=4
    )
    
    plt.show()

# =====================
# EXECUTION HANDLERS (Critical Fix)
# =====================
def run_backtest(df_tuple):
    """Process backtest data with proper tuple handling"""
    # Extract DataFrame from tuple or use directly
    if isinstance(df_tuple, tuple):
        df = df_tuple[0]  # Use first DataFrame from tuple
    else:
        df = df_tuple  # Use DataFrame directly if not a tuple
    
    # Convert index to datetime if needed
    if not isinstance(df.index, pd.DatetimeIndex):
        df = df.reset_index()
        df['date'] = pd.to_datetime(df['date'])
        df = df.set_index('date').sort_index()
    
    # Process each timestamp
    for current_dt in df.index:
        try:
            # Get row data using loc to avoid tuple indexing issues
            row = df.loc[current_dt]
            
            # Entry condition check
            if check_entry_conditions(df, current_dt):
                position_size = min(
                    engine.balance * 0.02,
                    engine.balance - 100,
                    key=lambda x: x if x > 0 else 0
                )
                
                if row.close <= 0 or pd.isna(row.close):
                    logger.error(f"Skipping invalid price at {current_dt}")
                    continue
                    
                shares = max(0.01, position_size / row.close)
                trade = engine.execute_trade(symbol, row.close, shares, 'BUY', current_dt)
                log_trade(trade, symbol)
            
            # Exit condition check
            if engine.current_position and check_exit_conditions(df, current_dt):
                trade = engine.execute_trade(
                    symbol, 
                    row.close,
                    engine.current_position['shares'],
                    'SELL',
                    current_dt
                )
                log_trade(trade, symbol)
                
        except Exception as e:
            logger.error(f"Error processing {current_dt}: {str(e)}")
            continue

    return df

# =====================
# MAIN EXECUTION
# =====================
def main():
    ib = IB()
    ib.connect('127.0.0.1', 7497, clientId=1)
    try:
        # Backtest mode
        daily_data = get_historical_data(ib, symbol, backtest=True)
        run_backtest(daily_data)
        plot_candlestick(daily_data, backtest=True)
        
        # Real-time mode
        df_5m, df_30m, df_1h = get_historical_data(ib, symbol)
        plot_candlestick(df_5m)
        
    except Exception as e:
        logger.error(f"Execution error: {str(e)}")
    finally:
        if 'ib' in locals() and ib.isConnected():
            ib.disconnect()
            
            if engine.trade_history:
                trade_history = pd.DataFrame(engine.trade_history)
                
                # Calculate performance metrics
                total_trades = len(trade_history)
                profitable_trades = (trade_history['pnl'] > 0).sum()
                losing_trades = (trade_history['pnl'] < 0).sum()
                win_rate = (trade_history['pnl'] > 0).mean() * 100
                
                # Calculate returns and risk metrics
                total_pnl = trade_history['pnl'].sum()
                returns = trade_history['pnl'] / initial_balance
                annualized_return = ((engine.balance/initial_balance) ** (252/len(returns)) - 1) * 100
                volatility = returns.std() * np.sqrt(252)
                
                # Calculate Sharpe and Sortino ratios
                risk_free_rate = 0.02  # 2% annual risk-free rate
                excess_returns = returns - (risk_free_rate/252)
                sharpe_ratio = np.sqrt(252) * excess_returns.mean() / returns.std()
                downside_returns = returns[returns < 0]
                sortino_ratio = np.sqrt(252) * excess_returns.mean() / downside_returns.std()
                
                # Calculate drawdown
                cumulative_returns = (1 + returns).cumprod()
                rolling_max = cumulative_returns.expanding().max()
                drawdowns = (cumulative_returns - rolling_max) / rolling_max
                max_drawdown = drawdowns.min() * 100
                
                # Calculate trade statistics
                avg_profit = trade_history[trade_history['pnl'] > 0]['pnl'].mean()
                avg_loss = abs(trade_history[trade_history['pnl'] < 0]['pnl'].mean())
                profit_factor = abs(trade_history[trade_history['pnl'] > 0]['pnl'].sum()) / abs(trade_history[trade_history['pnl'] < 0]['pnl'].sum())
                
                logger.info(f"Total Trades: {total_trades}")
                logger.info(f"Profitable Trades: {profitable_trades}")
                logger.info(f"Losing Trades: {losing_trades}")
                logger.info(f"Win Rate: {win_rate:.2f}%")
                logger.info(f"Total PnL: ${total_pnl:.2f}")
                logger.info(f"Annualized Return: {annualized_return:.2f}%")
                logger.info(f"Volatility: {volatility:.2f}")
                logger.info(f"Sharpe Ratio: {sharpe_ratio:.2f}")
                logger.info(f"Sortino Ratio: {sortino_ratio:.2f}")
                logger.info(f"Max Drawdown: {max_drawdown:.2f}%")
                logger.info(f"Average Profit: ${avg_profit:.2f}")
                logger.info(f"Average Loss: ${avg_loss:.2f}")
                logger.info(f"Profit Factor: {profit_factor:.2f}")

                # In main() function:
                start_date = trade_history['timestamp'].min().to_pydatetime().astimezone(pytz.UTC)
                end_date = trade_history['timestamp'].max().to_pydatetime().astimezone(pytz.UTC)
                plot_pnl_calendar(engine, start_date, end_date)

if __name__ == '__main__':
    main()
