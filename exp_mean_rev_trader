# =====================
# CORE IMPORTS
# =====================
from ib_insync import *
import pandas as pd
import numpy as np
import mplfinance as mpf
from datetime import datetime, timedelta, time
import asyncio
import nest_asyncio
import logging
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from time import sleep
from matplotlib.colors import ListedColormap
from collections.abc import MutableMapping  # FIX: Added correct abstract base class import
import pytz
import re

# =====================
# LINKED LIST DICTIONARY IMPLEMENTATION
# =====================
class LLNode:
    __slots__ = ('key', 'value', 'next')
    def __init__(self, key, value, next=None):
        self.key = key
        self.value = value
        self.next = next

class LLDict(MutableMapping):  # FIX: Replaced DictAbstract with MutableMapping
    def __init__(self):
        self.head = None
        self._size = 0

    def __setitem__(self, key, value):
        current = self.head
        while current is not None:
            if current.key == key:
                current.value = value
                return
            current = current.next
        self.head = LLNode(key, value, self.head)
        self._size += 1

    def __getitem__(self, key):
        current = self.head
        while current is not None:
            if current.key == key:
                return current.value
            current = current.next
        raise KeyError(key)

    def __delitem__(self, key):
        prev = None
        current = self.head
        while current is not None:
            if current.key == key:
                if prev:
                    prev.next = current.next
                else:
                    self.head = current.next
                self._size -= 1
                return
            prev = current
            current = current.next
        raise KeyError(key)

    def __iter__(self):
        current = self.head
        while current is not None:
            yield current.key
            current = current.next

    def __len__(self):
        return self._size

# =====================
# GLOBAL CONFIGURATION
# =====================
symbol = 'GOOGL' # Random Stock Ticker Generator - API call IBKR random pos in array of NYSE/NASDAQ tickers list?
initial_balance = 10000
MIN_REQUIRED_BANDS = 2
gradient_levels = [0.5, 1.0, 1.5, 2.0]  # Adjust these multipliers as needed
colors = ['#4B0082', '#6A0DAD', '#8A2BE2', '#9370DB']  # Gradient colors

# =====================
# MARKET CLOSURE WARNING
# =====================
def is_market_close(index):
    """Check if current time is near market close (timezone-aware version)"""
    ny_tz = pytz.timezone('America/New_York')
    
    # Convert to NY timezone if not already localized
    if index.tz is None:
        index = index.tz_localize(ny_tz)  # Changed from tz_convert to tz_localize
    else:
        index = index.tz_convert(ny_tz)
    
    current_time = index.time
    market_close_time = time(16, 0)  # 4 PM ET
    time_delta = timedelta(minutes=10)
    
    # Create aware datetime objects for comparison
    dummy_date = index.date()
    market_close_dt = ny_tz.localize(datetime.combine(dummy_date, market_close_time))
    
    return (market_close_dt - time_delta) <= index <= market_close_dt

# =====================
# PNL CALENDAR (FIXED)
# =====================
def plot_pnl_calendar(engine, start_date, end_date, initial_balance, symbol):
    ny_tz = pytz.timezone('America/New_York')
    
    # Convert trade history to DataFrame FIRST
    trade_df = pd.DataFrame(engine.trade_history)
    
    # Safely get start/end dates
    if not trade_df.empty and 'timestamp' in trade_df.columns:
        trade_df['timestamp'] = pd.to_datetime(trade_df['timestamp'])
        trade_df['timestamp'] = trade_df['timestamp'].dt.tz_convert('America/New_York')
        
        start_date = trade_df['timestamp'].min().normalize()
        end_date = trade_df['timestamp'].max().normalize()
    else:
        logger.warning("No trades available for calendar")
        return

    # Processing loop with proper timezone handling
    current_date = start_date.normalize()
    while current_date <= end_date:
        # Date calculations remain unchanged
        next_month = current_date.replace(day=1) + pd.DateOffset(months=1)
        month_end = min(next_month - pd.Timedelta(days=1), end_date)
        all_dates = pd.date_range(start=current_date, end=month_end, freq='D', tz=ny_tz)

        # Daily PNL processing with explicit timezone
        daily_pnl = pd.DataFrame.from_dict(engine.daily_pnl, orient='index', columns=['pnl'])
        daily_pnl.index = pd.to_datetime(daily_pnl.index).tz_convert(ny_tz)
        daily_pnl = daily_pnl.reindex(all_dates, fill_value=0).reset_index()
        daily_pnl.rename(columns={'index': 'date'}, inplace=True)

        # Visualization setup remains the same
        fig, ax = plt.subplots(figsize=(16, 28))
        ax.set_facecolor('#f0f0f0')
        
        # Date positioning logic unchanged
        first_day_of_month = current_date.replace(day=1)
        first_day_offset = first_day_of_month.weekday()
        days_in_month = (current_date + pd.offsets.MonthEnd(1)).day
        num_weeks = ((days_in_month + first_day_offset - 1) // 7) + 1

        # Modified trade filtering using DataFrame
        for i, row in daily_pnl.iterrows():
            date = row['date']
            col = date.weekday()
            day_of_month = date.day
            week_num = (day_of_month + first_day_offset - 1) // 7
            row_pos = -week_num

            # Weekend handling remains
            if col in [5, 6]:
                color = '#000000'
                ax.add_patch(plt.Rectangle(
                    (col, row_pos), 1, 1,
                    facecolor=color,
                    edgecolor='gray',
                    lw=0.5
                ))
            else:
                # FIX #2 GOES HERE: Proper trade filtering
                # Convert both timestamps to date for comparison
                day_trades = trade_df[
                    trade_df['timestamp'].dt.date == date.date()
                ].to_dict('records')
                
                metrics = calculate_day_metrics(day_trades)
                
                # Color logic remains unchanged
                if metrics['num_trades'] == 0:
                    color = '#d3d3d3'
                else:
                    max_abs_pnl = abs(daily_pnl['pnl']).max()
                    color_intensity = min(1, abs(row.pnl) / max_abs_pnl) if max_abs_pnl > 0 else 0
                    color = f"#{int((0.8-color_intensity)*255):02x}ff{int((0.8-color_intensity)*255):02x}"

                ax.add_patch(plt.Rectangle(
                    (col, row_pos), 1, 1,
                    facecolor=color,
                    edgecolor='gray',
                    lw=0.5
                ))

                ax.text(col + 0.05, row_pos + 0.85, str(day_of_month), 
                       ha='left', va='top', fontsize=8)
                ax.text(col + 0.5, row_pos + 0.5,
                       f"${row.pnl:.2f}\nW%: {metrics['win_rate']:.2f}%\nTrades: {metrics['num_trades']}",
                       ha='center', va='center', fontsize=6)

        # Axis configuration remains
        ax.set_xlim(-0.5, 6.5)
        ax.set_ylim(-num_weeks - 0.5, 0.5)
        ax.set_xticks(range(7))
        ax.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
        ax.set_yticks([])

        plt.suptitle(f"{symbol} Trading Performance {current_date.strftime('%B %Y')}", 
                    fontsize=16)
        plt.tight_layout()
        plt.show()

        current_date = next_month

def calculate_day_metrics(trades):
    """Calculate trading metrics with proper error handling"""
    num_trades = len(trades)
    wins = 0
    losses = 0
    pnl = 0.0
    
    for t in trades:
        # Add type check and conversion
        if isinstance(t, dict) and 'timestamp' in t:
            ts = pd.to_datetime(t['timestamp'])
        else:
            logger.error("Invalid trade format in calculate_day_metrics")
            continue
        trade_pnl = t.get('pnl', 0.0)
        pnl += float(trade_pnl)
        
        if trade_pnl > 0:
            wins += 1
        elif trade_pnl < 0:
            losses += 1

    # FIXED CONTINUATION
    win_rate = (wins / num_trades * 100) if num_trades > 0 else 0.0
    win_loss_ratio = (wins / losses) if losses > 0 else float('inf')
    
    return {
        'win_rate': round(win_rate, 2),
        'win_loss_ratio': win_loss_ratio if win_loss_ratio < float('inf') else np.inf,
        'num_trades': num_trades,
        'pnl': round(pnl, 2)
    }

    
# =====================
# BACKTESTING ENGINE
# =====================
# Added to BacktestEngine
@property
def max_position_size(self):
    return self.balance * 0.25  # Never risk >25% of capital

class BacktestEngine:
    def __init__(self, mode='neutral'):
        self.mode = mode
        self.mode_config = {
            'aggressive': {
                'rsi_entry': 28, 'williams_entry': -85, 'position_risk': 0.03,
                'volatility_mult': 1.8, 'profit_target': 1.8, 'stop_loss': 0.96,
                'adx_threshold': 22, 'di_ratio': 1.25, 'macd_crossover': 1.15
            },
            'neutral': {
                'rsi_entry': 32, 'williams_entry': -80, 'position_risk': 0.02,
                'volatility_mult': 1.5, 'profit_target': 1.5, 'stop_loss': 0.94,
                'adx_threshold': 25, 'di_ratio': 1.35, 'macd_crossover': 1.05
            },
            'conservative': {
                'rsi_entry': 35, 'williams_entry': -75, 'position_risk': 0.01,
                'volatility_mult': 1.2, 'profit_target': 1.2, 'stop_loss': 0.92,
                'adx_threshold': 28, 'di_ratio': 1.5, 'macd_crossover': 1.0
            }
        }
        self.validate_mode_config()
        # Initialize other engine components
        self.current_row = None  # Track current market data
        self.balance = initial_balance
        self.positions = LLDict()
        self.trade_history = []
        self.current_position = None
        self.fee_per_share = 0.0035
        self.slippage = 0.0002
        self.volatility_filter = {'min_mrc': 0.02, 'max_mrc': 0.05}
        self.trade_markers = []
        self.daily_pnl = {}
        self.entry_conditions = {
            'rsi_threshold': False,
            'position_status': False
        }
        self.williams_params = {
            'lookback': 30,
            'oversold': -80,
            'overbought': -20,
            'confirmation_bars': 3
        }
        self.indicator_weights = {
            'rsi': 0.25,
            'williams': 0.25,
            'macd': 0.2,
            'vwap': 0.15,
            'adx': 0.1,
            'volume': 0.05  # New component
        }
        self.volatility_filter = {
            'min_mrc': 0.02,
            'max_mrc': 0.05    
        }
        self.adx_params = {
            'mean_reversion_threshold': 25,
            'low_volatility_threshold': 15,  # New filter
            'trend_strength_exit': 40,
            'rapid_trend_threshold': 15
        }
        
        if self.current_position and self.current_position['timestamp'].tz is None:
            raise ValueError("Position timestamps must be timezone-aware")
    
    def validate_mode_config(self):
        required_keys = ['rsi_entry', 'williams_entry', 'position_risk',
                        'volatility_mult', 'profit_target', 'stop_loss',
                        'adx_threshold', 'di_ratio', 'macd_crossover']
        for mode, config in self.mode_config.items():
            missing = [k for k in required_keys if k not in config]
            if missing:
                raise ValueError(f"Missing config keys in {mode}: {missing}")

    # =====================
    # OPTIMIZED POSITION SIZING
    # =====================
    def calculate_position_size(self, row):
        mp = self.mode_config[self.mode]
        risk_capital = self.balance * mp['position_risk']
        combined_vol = (
            max(0.01, row['meanrange']) * mp['volatility_mult'] +  # Prevent zero-division
            row['ATR']
        ) / 2
    
    

    def composite_signal_score(self, row):
        """Dynamic volatility scaling with regime detection"""
        # Volatility regime detection (1-3x multiplier)
        volatility_regime = 1 + min(2, (row['ATR'] / row['close']) * 2.5)
        
        # Component scores with dynamic bounds
        scores = {
            'rsi': max(-1, min(1, (35 - row['RSI']) / 15)),  # More sensitive to oversold
            'williams': max(-1, min(1, (-85 - row['Williams_%R']) / 40)),
            'macd': self._normalize_macd(row['MACD'], row['Signal']),
            'vwap': (row['VWAP'] - row['close']) / row['VWAP'] * 2.2,
            'adx': self._dynamic_adx_weight(row['adx'], row['+di'], row['-di']),
            'volume': (row['volume'] - row['volume_ma']) / row['volume_ma'] * 1.2
        }
        
        return sum(
            self.indicator_weights[k] * scores[k] * volatility_regime
            for k in self.indicator_weights
        )

    def _normalize_macd(self, macd, signal):
        """Safe MACD normalization with signal consideration"""
        denominator = abs(macd) + abs(signal)
        if denominator < 1e-8:
            return 0
        return (macd - signal) / denominator

    def _dynamic_adx_weight(self, adx, plus_di, minus_di):
        """ADX weighting that considers trend direction and strength"""
        di_diff = abs(plus_di - minus_di)
        trend_strength = max(0, (25 - min(adx, 25)) / 25)  # 0-1 based on ADX <25
        direction_confidence = 1 - (di_diff / 100)  # 1 when DIs converge
        return trend_strength * direction_confidence

    def apply_slippage(self, price, is_buy):
        """
        Applies slippage to the execution price
        """
        slippage_factor = 1 + (self.slippage if is_buy else -self.slippage)
        return price * slippage_factor
    
    def calculate_fees(self, shares):
        """
        Calculates trading fees based on number of shares
        """
        return abs(shares * self.fee_per_share)

    # =====================
# TRADE EXECUTION CORE
# =====================
def execute_entry(price, timestamp, position_size):
    """Timezone-validated trade entry with position checks"""
    # Convert and validate timestamp
    timestamp = enforce_ny_timezone(pd.to_datetime(timestamp))
    
    # Calculate position size with sanity checks
    shares = max(0.01, position_size / price)
    if not np.isfinite(shares):
        logger.error(f"Invalid share calculation: {position_size}/{price}")
        return None

    # Execute trade with timezone-safe timestamp
    trade = engine.execute_trade(
        symbol=symbol,
        price=price,
        shares=shares,
        action='BUY',
        timestamp=timestamp
    )
    
    if trade:
        log_trade(trade)
        return trade
    return None

def execute_exit(price, timestamp):
    """Validated position closing with volatility checks"""
    if not engine.current_position:
        logger.warning("Exit attempted with no open position")
        return None

    # Convert and validate timestamp
    timestamp = enforce_ny_timezone(pd.to_datetime(timestamp))
    
    # Get current position details
    position = engine.current_position
    exit_shares = min(position['shares'], engine.max_position_size)
    
    # Execute trade with timezone validation
    trade = engine.execute_trade(
        symbol=symbol,
        price=price,
        shares=exit_shares,
        action='SELL',
        timestamp=timestamp
    )
    
    if trade:
        log_trade(trade)
        return trade
    return None

# Initialize engine globally
engine = BacktestEngine()

# =====================
# CORRECTED ENTRY/EXIT CONDITIONS (FIXED LOOK AHEAD)
# =====================
# =====================
# CONDITION CHECKS (FIXED LOOKAHEAD)
# =====================
def check_entry_conditions(df, index):
    """Use SHIFTED VALUES for all indicators with timezone validation"""
    # Validate timestamp first
    if str(index.tz) != 'America/New_York':
        raise ValueError(f"Invalid timezone in entry check: {index.tz}")
    
    try:
        row = df.loc[index]
    except KeyError:
        logger.error(f"Missing data at {index}")
        return False

    # Core entry logic
    entry_confluence = (
        (row['close_shifted'] < row['loband1_shifted']) and  # Changed
        (row['RSI_shifted'] < 32) and
        (row['Williams_%R_shifted'] < -80) and
        (row['VWAP_shifted'] > row['close_shifted']) and 
        (row['MACD_shifted'] < row['Signal_shifted']) and
        (row['adx_shifted'] < engine.adx_params['mean_reversion_threshold']) and
        (row['+di_shifted'] < row['-di_shifted']) and
        (row['%K_shifted'] < row['%D_shifted']) and
        (row['%D_shifted'] < 30)
    )
    
    # Volatility checks
    volatility_ok = (
        engine.volatility_filter['min_mrc'] < row['meanrange'] < 
        engine.volatility_filter['max_mrc']
    )
    
    # Position safety
    position_ok = (
        engine.composite_signal_score(row) > 0.7 and
        engine.balance > 1000 and
        not engine.current_position and
        volatility_ok
    )
    
    # Temporal validation
    time_ok = not is_market_close(index)
    
    return entry_confluence and position_ok and time_ok

def check_exit_conditions(df, index):
    """Exit logic with shifted values and timezone safety"""
    if str(index.tz) != 'America/New_York':
        raise ValueError(f"Invalid timezone in entry check: {index.tz}")
        
    if not engine.current_position:
        return False

    try:
        row = df.loc[index]
    except KeyError:
        logger.error(f"Missing exit data at {index}")
        return False

    entry_price = engine.current_position['entry_price']
    
    # Profit conditions
    profit_conditions = (
        row['close_shifted'] > row['SMA_20_shifted'] or
        row['RSI_shifted'] > 40 or
        row['Williams_%R_shifted'] > -60 or
        row['close_shifted'] > row['VWAP_shifted'] or
        row['adx_shifted'] > 40 or
        row['+di_shifted'] > row['-di_shifted']
    )
    
    # Risk management
    stop_loss_conditions = (
        row['close'] < entry_price * 0.98 or
        row['meanrange'] > engine.volatility_filter['max_mrc'] * 1.5 or
        np.abs(row['adx_shifted'] - row['adx_shifted'].shift(3)) > 15
    )

    return profit_conditions or stop_loss_conditions
    
# ENHANCED LOGGING WITH PROPER COLORIZATION
# =====================
class TradeFormatter(logging.Formatter):
    # ANSI color codes
    GREEN = '\033[92m'
    RED = '\033[91m'
    WHITE = '\033[0m'
    
    def format(self, record):
        message = super().format(record)
        message = self.colorize_actions(message)
        message = self.colorize_values(message)
        return message

    def colorize_actions(self, message):
        """Color-code BUY/SELL tags with proper regex"""
        return re.sub(
            r'^(BUY|SELL)(\s+\|)',
            lambda m: (f"{self.GREEN if m.group(1) == 'BUY' else self.RED}"
                      f"{m.group(1)}{self.WHITE}{m.group(2)}"),
            message,
            count=1
        )
        
    def colorize_values(self, message):
        """Apply color based on numeric values with proper handling"""
        def color_repl(match):
            value_str = match.group(1) or match.group(2)
            try:
                value = float(value_str)
                color = self.GREEN if value > 0 else self.RED
            except:
                color = self.WHITE
            return f"{color}{match.group(0)}{self.WHITE}"

        # Split message into components and skip Price/Size fields
        parts = message.split(' | ')
        for i, part in enumerate(parts):
            if part.startswith(('Price:', 'Size:')):
                continue  # Skip already-colored fields
            parts[i] = re.sub(
                r'\$ *(-?\d+\.\d+)|(-?\d+\.\d+)%',
                color_repl,
                part
            )
        return ' | '.join(parts)

# Configure root logger
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)  # Changed to DEBUG for verbose logging

# Clear existing handlers
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)

# Create and configure handlers
console_handler = logging.StreamHandler()
console_handler.setFormatter(TradeFormatter('%(asctime)s | %(message)s'))
file_handler = logging.FileHandler('strategy_execution.log')
file_handler.setFormatter(logging.Formatter('%(asctime)s | %(message)s'))

root_logger.addHandler(console_handler)
root_logger.addHandler(file_handler)
logger = logging.getLogger(__name__)

# =====================
# MODIFIED LOG_TRADE FUNCTION
# =====================
def log_trade(trade, symbol):  # Added symbol param:
    """Robust logging with improved color handling"""
    try:
        # Convert trade timestamp properly
        timestamp = trade['timestamp']
        if timestamp.tz is None:  # Handle naive timestamps
            timestamp = timestamp.tz_localize('UTC')
        trade_time = timestamp.tz_convert('America/New_York')
        
        # Use .floor() instead of .date for timezone-aware datetimes
        trade_day = trade_time.floor('D')  
        
        # Build log message with direct color formatting
        log_parts = [
                    f"{TradeFormatter.GREEN if trade['action'] == 'BUY' else TradeFormatter.RED}"
                    f"{trade['action'].ljust(6)}\033[0m | {symbol.ljust(5)}",
                    f"Time:  {trade_time.strftime('%Y-%m-%d %H:%M:%S%z')}",
                    f"Price: \033[97m${trade['price']:7.2f}\033[0m",  # Force white
                    f"Size:  \033[97m${trade['position_value']:7.2f}\033[0m"  # Force white
                    f"Shares: \033[97m{trade['shares']:>6.2f}\033[0m"  # White text
                ]
        
        if trade['action'] == 'SELL':
            # Direct ANSI color codes for PnL
            pnl_color = TradeFormatter.GREEN if trade['pnl'] > 0 else TradeFormatter.RED
            log_parts.extend([
                f"PnL:   {pnl_color}${trade['pnl']:7.2f}{TradeFormatter.WHITE}",
                f"Return: {pnl_color}{trade['pnl_pct']:6.2f}%{TradeFormatter.WHITE}"
            ])
            
        logger.info(" | ".join(log_parts))

    except Exception as e:
        logger.error(f"Failed to log trade: {str(e)}", exc_info=True)

# =====================
# DATA MANAGEMENT (UPDATED FOR MRC)
# =====================
nest_asyncio.apply()

import matplotlib.pyplot as plt
import numpy as np

def calculate_mrc_with_gradient(df: pd.DataFrame, length=100, gradient_levels=[1.0, 2.0]):
    """
    Robust Mean Reversion Channel with Gradient Band Generation
    """
    # =====================
    # 1. Data Validation
    # =====================
    min_required_length = max(150, int(length * 1.5))
    if len(df) < min_required_length:
        raise ValueError(f"Need minimum {min_required_length} data points (current: {len(df)})")
    
    # =====================
    # 2. Core Calculations
    # =====================
    # HLC3 with forward/backward filling
    df['hlc3'] = (df['high'].ffill().bfill() + 
                 df['low'].ffill().bfill() + 
                 df['close'].ffill().bfill()) / 3
    
    # Meanline calculation with safety
    df['meanline'] = supersmoother(df['hlc3'], length).bfill().ffill()
    
    # True Range with volatility floor
    tr = np.maximum(
        df['high'] - df['low'],
        np.maximum(
            abs(df['high'] - df['close'].shift().bfill()),
            abs(df['low'] - df['close'].shift().bfill())
        )
    ).clip(lower=0.0001)
    
    # Mean range calculation with fallback
    try:
        df['meanrange'] = supersmoother(tr, length).bfill().ffill()
    except Exception as e:
        df['meanrange'] = tr.rolling(20).mean().bfill().ffill()
    
    df['meanrange'] = df['meanrange'].replace(0, 0.0001)
    
    # =====================
    # 3. Band Generation
    # =====================
    required_bands = ['meanline', 'upband1', 'loband1', 'upband2', 'loband2']
    
    # Force band creation using first 2 gradient levels
    for i, level in enumerate(gradient_levels[:2], 1):
        df[f'upband{i}'] = df['meanline'] + (df['meanrange'] * level).bfill().ffill()
        df[f'loband{i}'] = df['meanline'] - (df['meanrange'] * level).bfill().ffill()
    
    # =====================
    # 4. Post-Validation
    # =====================
    missing = [col for col in required_bands if col not in df.columns]
    if missing:
        # Fallback to close-based bands
        df['meanline'] = df.get('meanline', df['close'])
        base_vol = df['meanrange'].mean() or df['close'].std()
        
        for i in [1, 2]:
            df[f'upband{i}'] = df['meanline'] + (base_vol * gradient_levels[i-1])
            df[f'loband{i}'] = df['meanline'] - (base_vol * gradient_levels[i-1])
    
    return df


def plot_mrc_gradient(ax, df, gradient_levels, colors):
    """
    Plots MRC gradient shading on a given axis.
    :param ax: Matplotlib axis to plot on.
    :param df: DataFrame containing MRC bands.
    :param gradient_levels: List of deviation multipliers for gradient.
    :param colors: List of colors for each gradient level.
    """
    for i in range(len(gradient_levels) - 1):
        upper_band = df[f'upband_{gradient_levels[i]}']
        lower_band = df[f'loband_{gradient_levels[i]}']
        next_upper_band = df[f'upband_{gradient_levels[i + 1]}']
        next_lower_band = df[f'loband_{gradient_levels[i + 1]}']

        # Plot filled area between bands with gradient color
        ax.fill_between(
            df.index,
            lower_band,
            next_lower_band,
            color=colors[i],
            alpha=0.5,
            edgecolor=None
        )
        ax.fill_between(
            df.index,
            upper_band,
            next_upper_band,
            color=colors[i],
            alpha=0.5,
            edgecolor=None
        )


def get_historical_data(ib, symbol, exchange='SMART', currency='USD', backtest=False):
    contract = Stock(symbol, exchange, currency)
    ib.qualifyContracts(contract) 
    if backtest:
        bars = ib.reqHistoricalData(
            contract,
            endDateTime='',
            durationStr='30 D',
            barSizeSetting='1 hour',
            whatToShow='TRADES',
            useRTH=True,
            formatDate=1,
            keepUpToDate=False
        )
        
        if not bars:
            raise ValueError("No historical data received")
        
                # CREATE DF FIRST
        df = util.df(bars)
        if df.empty:
            raise ValueError("Empty DataFrame received from IB")

        # THEN MODIFY INDEX
        df.index = pd.to_datetime(df.index, utc=True).tz_convert('America/New_York')
        
           # Ensure core price columns exist
        price_columns = ['high', 'low', 'close']
        for col in price_columns:
            if col not in df.columns:
                df[col] = df['close'].ffill().bfill()  # Create from close if missing
    
        # Calculate HLC3 first before MRC
        df['hlc3'] = (df['high'] + df['low'] + df['close']) / 3
        df = calculate_mrc_with_gradient(df, length=100)

        df['loband1_shifted'] = df['loband1'].shift(1)
        df['upband1_shifted'] = df['upband1'].shift(1)
        
        # Clean column names first
        df = df.rename(columns={
            'open': 'open',
            'high': 'high',
            'low': 'low', 
            'close': 'close',
            'volume': 'volume'
        }).drop(columns=['average', 'barCount'], errors='ignore')

        def safe_tz_convert(series):
            if series.dt.tz is None:
                return series.dt.tz_localize('UTC').dt.tz_convert('America/New_York')
            else:
                return series.dt.tz_convert('America/New_York')

        df['date'] = safe_tz_convert(pd.to_datetime(df['date']))
    
        # Add post-calculation validation
        required_mrc_cols = ['meanline', 'upband1', 'loband1', 'upband2', 'loband2']
        missing = [col for col in required_mrc_cols if col not in df.columns]
        if missing:
            logger.warning(f"MRC fallback activated for: {missing}")
            for col in missing:
                df[col] = df['close']  # Temporary fallback
        
        # Technical indicators
        adx_components = calculate_adx(df)
        df = df.join(adx_components, lsuffix='', rsuffix='_drop').drop(columns=['adx_drop', '+di_drop', '-di_drop', 'adx_shifted_drop'], errors='ignore')
        df[['adx', '+di', '-di']] = df[['adx', '+di', '-di']].fillna(0)

        # Stochastic oscillator
        df = stochastic_oscillator(df)
        
        # VWAP calculation
        df['vwap_numerator'] = df['volume'] * df['hlc3']  # Use existing hlc3
        df['vwap_denominator'] = df['volume']
        df['trading_date'] = df.index.date
        df['VWAP'] = (df.groupby('trading_date')['vwap_numerator'].cumsum() /
                     df.groupby('trading_date')['vwap_denominator'].cumsum())
        
    try:
        df = calculate_mrc_with_gradient(df, length=100)
    except Exception as e:
        logger.error(f"MRC Failure: {str(e)}")
    
    # Guaranteed band initialization
    band_defaults = {
        'meanline': df['close'],
        'upband1': df['close'] * 1.01,
        'loband1': df['close'] * 0.99,
        'upband2': df['close'] * 1.02,
        'loband2': df['close'] * 0.98
    }
    
    for band in band_defaults:
        if band not in df.columns:
            df[band] = band_defaults[band]
    
    # Final validation checkpoint
        assert all(b in df.columns for b in band_defaults), "Band fallback failed"
    
        # MACD with noise filtering
        exp12 = df['close'].ewm(span=12, adjust=False).mean()
        exp26 = df['close'].ewm(span=26, adjust=False).mean()
        df['MACD'] = np.where(np.abs(exp12 - exp26) < 1e-8, 0, exp12 - exp26)
        df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
        df['Histogram'] = df['MACD'] - df['Signal']
        
        # Calculate RSI before stochastic oscillator
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0.0)
        loss = -delta.where(delta < 0, 0.0)
        avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()
        avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()        
        df['RSI'] = 100 - (100 / (1 + (avg_gain / avg_loss)))

        # Now calculate stochastic oscillator (which uses RSI)
        df = stochastic_oscillator(df)
        
        # Williams %R
        df['Williams_%R'] = williams_r(df['high'], df['low'], df['close'])
        df['Williams_%R_diff'] = df['Williams_%R'].diff(3).fillna(0)
        
        df['ATR'] = df['high'] - df['low']  # Basic implementation - replace with proper ATR
        df['volume_ma'] = df['volume'].rolling(20).mean()
        
        # Feature engineering updates
        df = df.drop(['vwap_numerator', 'vwap_denominator', 'trading_date'], axis=1)
        
        # ADD SHIFTED COLUMNS AFTER INDICATOR CALCULATION
        df['Volume_MA_shifted'] = df['volume'].rolling(20).mean().shift(1)
        df['RSI_shifted'] = df['RSI'].shift(1)
        df['Williams_%R_shifted'] = df['Williams_%R'].shift(1)
        df['VWAP_shifted'] = df['VWAP'].shift(1)
        df['MACD_shifted'] = df['MACD'].shift(1)
        df['Signal_shifted'] = df['Signal'].shift(1)
        df['adx_shifted'] = df['adx'].shift(1)
        df['+di_shifted'] = df['+di'].shift(1)
        df['-di_shifted'] = df['-di'].shift(1)
        df['%K_shifted'] = df['%K'].shift(1)
        df['%D_shifted'] = df['%D'].shift(1)
        df['loband1_shifted'] = df['loband1'].shift(1)
        df['upband1_shifted'] = df['upband1'].shift(1)
        df['close_shifted'] = df['close'].shift(1)
        scaling_factor = 1.0  # Adjust based on your data
        df['meanrange'] *= scaling_factor
        
        # Validate core indicators for composite score
        composite_required = ['ATR', 'volume_ma', 'adx', 'RSI', 'Williams_%R', 'MACD', 'VWAP']
        missing = [col for col in composite_required if col not in df.columns]
        if missing:
            raise ValueError(f"Missing composite score columns: {missing}")

        # Cleanup MRC bands only if actually used elsewhere

        mrc_bands = ['meanline', 'upband1', 'loband1', 'upband2', 'loband2']
        df[mrc_bands] = df[mrc_bands].ffill().bfill()
        
        if df.empty:
            raise ValueError("Empty DataFrame after MRC processing")
            
        assert not df[['adx', '+di', '-di'] + required_mrc_cols].isnull().values.any(), \
            "NaN values present in critical columns"
            
        required_mrc_cols = ['meanline', 'upband1', 'loband1', 'upband2', 'loband2']
        if not all(col in df.columns for col in required_mrc_cols):
            raise ValueError("MRC calculation failed - missing columns")
        
        return df
        
# =====================
# WILLIAMS %R IMPLEMENTATION
# =====================
def williams_r(high, low, close, lookback=14):
    """Calculate Williams %R with NaN handling"""
    highest_high = high.rolling(window=lookback, min_periods=1).max()
    lowest_low = low.rolling(window=lookback, min_periods=1).min()
    
    # Handle division by zero and NaN cases
    denominator = (highest_high - lowest_low).replace(0, np.nan)
    wr = -100 * ((highest_high - close) / denominator)
    return wr.fillna(-50)  # Neutral value when no range

# ======================
# STOCHASTIC OSCILLATOR
# ======================
def stochastic_oscillator(df, k_period=14, d_period=3):
    """Calculate Stochastic Oscillator with NaN handling"""
    low_min = df['low'].rolling(k_period).min()
    high_max = df['high'].rolling(k_period).max()
    
    df['%K'] = 100 * ((df['close'] - low_min) / (high_max - low_min))
    denominator = np.where(high_max != low_min, high_max - low_min, 1)
    df['%K'] = df['%K'].fillna(50)  # Neutral value when no range
    df['%D'] = df['%K'].rolling(d_period).mean()

    return df

# =====================
# ADX CALCULATION
# =====================
def calculate_adx(df, window=14):
    """Fixed ADX calculation without meanline overwrite"""
    high = df['high']
    low = df['low']
    close = df['close']
    
    # True Range calculation
    tr = np.maximum(
        high - low,
        np.maximum(
            np.abs(high - close.shift().bfill()),
            np.abs(low - close.shift().bfill())
        )
    )
    
    # Directional Movement
    up_move = high.diff()
    down_move = -low.diff()
    
    # Smoothed values
    tr_smooth = tr.ewm(alpha=1/window, adjust=False).mean()
    plus_dm = up_move.where((up_move > down_move) & (up_move > 0), 0.0)
    minus_dm = down_move.where((down_move > up_move) & (down_move > 0), 0.0)
    
    # Calculate DI values
    plus_di = 100 * (plus_dm.ewm(alpha=1/window, adjust=False).mean() / tr_smooth)
    minus_di = 100 * (minus_dm.ewm(alpha=1/window, adjust=False).mean() / tr_smooth)
    
    # DX and ADX calculation
    di_diff = np.abs(plus_di - minus_di)
    di_sum = (plus_di + minus_di).replace(0, 1e-8)
    dx = 100 * (di_diff / di_sum)
    adx = dx.ewm(alpha=1/window, adjust=False).mean().clip(0, 100)
    
    # Create shifted version
    df['adx'] = adx
    df['+di'] = plus_di
    df['-di'] = minus_di
    df['adx_shifted'] = df['adx'].shift(1).fillna(0)
    
    # REMOVED THE MEANLINE OVERWRITE HERE
    # Add during DataFrame initialization:
    df['buy_signal'] = False
    df['sell_signal'] = False
    
    return df[['adx', '+di', '-di', 'adx_shifted']]

# =====================
# SUPER SMOOTHER
# =====================
def supersmoother(src: pd.Series, length: int) -> pd.Series:
    if len(src) < 3:
        return src.ffill().bfill()  # Return original if insufficient data
    a1 = np.exp(-np.sqrt(2) * np.pi / length)
    b1 = 2 * a1 * np.cos(np.sqrt(2) * np.pi / length)
    c3 = -a1**2
    c2 = b1
    c1 = 1 - c2 - c3

    ss = np.zeros_like(src)
    ss[0] = src.iloc[0]
    ss[1] = src.iloc[1]
    for i in range(2, len(src)):
        ss[i] = c1 * src.iloc[i] + c2 * ss[i - 1] + c3 * ss[i - 2]
    
    return pd.Series(ss, index=src.index).bfill().ffill()

def sak_smoothing(series: pd.Series, filter_type: str, length: int) -> pd.Series:
    """Replaces existing technical indicators with unified filter"""
    cycle = 2 * np.pi / length
    alpha, beta, gamma = 0.0, 0.0, 0.0
    
    if filter_type == "Ehlers EMA":
        alpha = (np.cos(cycle) + np.sin(cycle) - 1) / np.cos(cycle)
        return series.ewm(alpha=alpha, adjust=False).mean()
    
    # Add other filter types as needed
    return series  # Fallback to original series

# =====================
# VISUALIZATION ENGINE
# =====================
def plot_candlestick(df, backtest=False):
    if df.empty:
        raise ValueError("Cannot plot empty DataFrame")
        # Standardize timezone first
    plot_df = df.copy()
    if plot_df.index.tz != pytz.timezone('America/New_York'):
        plot_df.index = plot_df.index.tz_convert('America/New_York')
    
    """Generates panels in correct order: Price, MACD, Volume, RSI, Williams %R, ADX, with MRC Gradient."""
    import matplotlib.pyplot as plt
    from matplotlib.patches import Patch

    plot_df = df.copy()
    if plot_df.index.tz is not None:
        plot_df.index = plot_df.index.tz_convert('America/New_York')
    else:
        plot_df.index = plot_df.index.tz_convert('America/New_York')
    # ===== REORDERED ADDPLOTS =====
    addplots = []

    # Buy/Sell Signals
    addplots.append(
        mpf.make_addplot(df['buy_signal'], type='scatter',
                         markersize=100, marker='^', color='lime', panel=0)
    )
    addplots.append(
        mpf.make_addplot(df['sell_signal'], type='scatter',
                         markersize=100, marker='v', color='red', panel=0)
    )

    # 1. Main Price Panel (0) - Add Lo-Hi Bands and Meanline
    addplots.extend([
        mpf.make_addplot(df["meanline"], panel=0, color="blue", width=1.5),
        mpf.make_addplot(df["upband1"], panel=0, color="darkorange", width=0.8),
        mpf.make_addplot(df["loband1"], panel=0, color="darkorange", width=0.8),
        mpf.make_addplot(df["upband2"], panel=0, color="red", width=0.8),
        mpf.make_addplot(df["loband2"], panel=0, color="red", width=0.8),
    ])

    # Style for the candlestick chart
    style = mpf.make_mpf_style(
        marketcolors=mpf.make_marketcolors(up="#006400", down="#8B0000"),
        gridstyle=":",
        gridcolor="gainsboro",
        facecolor="white",
    )

    # 2. MACD Panel (1)
    macd_hist_colors = np.where(df['Histogram'] >= 0, '#5cb85c', '#D83B2D')
    addplots.extend([
        mpf.make_addplot(df['MACD'], panel=1, color='#1f77b4', width=1.2, ylabel='MACD'),
        mpf.make_addplot(df['Signal'], panel=1, color='#ff7f0e', width=1.2),
        mpf.make_addplot(df['Histogram'], type='bar', panel=1,
                         color=macd_hist_colors, alpha=0.6, width=0.7)
    ])

    # 3. Volume Panel (2)
    volume_colors = np.where(df['close'] > df['open'], '#5cb85c', '#d9534f')
    addplots.append(mpf.make_addplot(df['volume'], type='bar', panel=2,
                                     color=volume_colors, alpha=0.6, ylabel='Volume'))

    # 4. RSI Panel (3)
    addplots.extend([
        mpf.make_addplot(df['RSI'], panel=3, color='#4B0082', width=1.2,
                         ylim=(0, 100), ylabel='RSI'),
        *[mpf.make_addplot(pd.Series(level, df.index), panel=3,
                           color='gray', linestyle='--', width=1.0) for level in [30, 50, 70]]
    ])

    # 5. Williams %R Panel (4)
    addplots.extend([
        mpf.make_addplot(df['Williams_%R'], panel=4,
                         color='blue', width=1.2, ylabel='Williams %R'),
        mpf.make_addplot(pd.Series(-20, df.index), panel=4,
                         color='red', linestyle='--', width=1.0),
        mpf.make_addplot(pd.Series(-80, df.index), panel=4,
                         color='green', linestyle='--', width=1.0)
    ])

    # 6. ADX Panel (5)
    addplots.extend([
        mpf.make_addplot(df['adx'], panel=5,
                         color='blue', width=1.5, ylabel='ADX/DIs'),
        mpf.make_addplot(df['+di'], panel=5,
                         color='green', width=1.5, alpha=0.75),
        mpf.make_addplot(df['-di'], panel=5,
                         color='red', width=1.5, alpha=0.75)
    ])

    # 7. Stochastic Oscillator Panel (6)
    addplots.append(mpf.make_addplot(df['%K'], panel=6,
                                     color='blue', width=1.2, ylabel='Stochastic'))
    addplots.append(mpf.make_addplot(df['%D'], panel=6,
                                     color='orange', width=1.2))
    addplots.extend([
        mpf.make_addplot(pd.Series(80, index=df.index), panel=6,
                         color='red', linestyle='--', width=1.0),
        mpf.make_addplot(pd.Series(20, index=df.index), panel=6,
                         color='green', linestyle='--', width=1.0)
    ])

    # ===== MRC GRADIENT HANDLING =====
    
    def plot_mrc_gradient(ax):
        """Plots gradient shading for MRC bands."""
        levels = sorted(gradient_levels, reverse=True)
        colors = ['#4B0082', '#6A5ACD', '#9370DB']  # Gradient colors
        
        for i in range(len(gradient_levels) - 1):
            upper_band = df[f'meanline'] + df[f'meanrange'] * gradient_levels[i]
            lower_band = df[f'meanline'] - df[f'meanrange'] * gradient_levels[i]
            next_upper_band = df[f'meanline'] + df[f'meanrange'] * gradient_levels[i + 1]
            next_lower_band = df[f'meanline'] - df[f'meanrange'] * gradient_levels[i + 1]

            ax.fill_between(
                df.index,
                lower_band,
                next_lower_band,
                color=colors[i],
                alpha=0.3
            )
            ax.fill_between(
                df.index,
                upper_band,
                next_upper_band,
                color=colors[i],
                alpha=0.3
            )

    
    assert not df[['meanline', 'upband1', 'loband1']].isnull().values.any(), "NaNs in MRC bands"
    assert isinstance(df.index, pd.DatetimeIndex), "Index is not datetime"
    
    # Create the figure and axes
    fig, axlist = mpf.plot(
        df,
        type='candle',
        style=style,
        addplot=addplots,
        volume=False,
        panel_ratios=(12, 6, 4, 4, 4, 4, 4),
        figsize=(14, 24),
        returnfig=True
    )
    

    # Apply MRC Gradient to the Main Price Panel (Panel Index: 0)
    main_ax = axlist[0]
    plot_mrc_gradient(main_ax)

    # ===== LEGEND HANDLING =====
    
    legends = {
        0: [("VWAP", "purple"), ("20 SMA", "blue"), ("MRC Bands", "orange")],
        1: [("MACD", "#1f77b4"), ("Signal", "#ff7f0e"), ("Histogram", "#5cb85c")],
        2: [("Volume (Up)", "#5cb85c"), ("Volume (Down)", "#d9534f")],
        3: [("RSI", "#4B0082"), ("30/70 Bands", "gray")],
        4: [("Williams %R", "blue"), ("-20 Overbought", "red"), ("-80 Oversold", "green")],
        5: [("ADX (Trend Strength)", "blue"),("+DI (Bullish Pressure)", "green"), ("-DI (Bearish Pressure)", "red")],
        6: [("Stochastic Oscillator", "blue")]
    }

    for panel_idx, ax in enumerate(axlist):
        if panel_idx in legends:
            legend_handles = [
                Patch(facecolor=color if isinstance(color, str) else None,
                      edgecolor="black",
                      label=name) for name, color in legends[panel_idx]
            ]
            ax.legend(handles=legend_handles,
                      loc="upper left",
                      fontsize="small",
                      frameon=True)

    logger.info(f"Loaded historical data with {len(df)} rows")
    plt.show()

# =====================
# TIMEZONE ENFORCEMENT
# =====================
def enforce_ny_timezone(dt_obj):
    """Universal timezone conversion with validation"""
    if isinstance(dt_obj, pd.DatetimeIndex):
        if dt_obj.tz is None:
            return dt_obj.tz_localize('UTC').tz_convert('America/New_York')
        return dt_obj.tz_convert('America/New_York')
    
    if isinstance(dt_obj, pd.Timestamp):
        if dt_obj.tz is None:
            return dt_obj.tz_localize('UTC').tz_convert('America/New_York')
        return dt_obj.tz_convert('America/New_York')
    
    raise TypeError(f"Unsupported type for timezone conversion: {type(dt_obj)}")

# =====================
# BACKTEST ENGINE MODIFICATIONS (FIXED LOOKAHEAD)
# =====================
def run_backtest(df):
     # Ensure datetime index
    if not isinstance(df.index, pd.DatetimeIndex):
        df.index = pd.to_datetime(df.index, errors='coerce')
    
    # Add timezone if missing
    if df.index.tz is None:
        df.index = df.index.tz_localize('UTC').tz_convert('America/New_York')
    # 1. Timezone Enforcement
    df.index = enforce_ny_timezone(df.index)
    
    # 2. Pre-backtest Checks
    required_columns = ['open', 'high', 'low', 'close', 'volume', 
                       'RSI', 'Williams_%R', 'VWAP', 'MACD', 'meanrange']
    missing = [col for col in required_columns if col not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")
        
    # 3. Main Execution Loop
    for i in range(1, len(df)):
        prev_dt = df.index[i-1]
        current_dt = df.index[i]
        
        try:
            # 4. Entry/Exit Logic
            if check_entry_conditions(df, prev_dt) and not engine.current_position:
                execute_entry(
                    price=df.loc[current_dt, 'open'],
                    timestamp=current_dt,
                    position_size=engine.calculate_position_size(df.loc[prev_dt])
                )
                
            if engine.current_position and check_exit_conditions(df, prev_dt):
                execute_exit(
                    price=df.loc[current_dt, 'open'],
                    timestamp=current_dt
                )
                
        except KeyError as e:
            logger.error(f"Missing data at {prev_dt}: {str(e)}")
            continue
            
    return df

# =====================
# MODE SELECTION INTERFACE
# =====================
def select_trading_mode():
    print("\nSelect Trading Strategy Mode:")
    print("1. Aggressive")
    print("2. Neutral (Default)")
    print("3. Conservative")
    
    choice = input("Enter choice (1-3): ").strip()
    modes = {
        '1': 'aggressive',
        '2': 'neutral',
        '3': 'conservative'
    }
    return modes.get(choice, 'neutral')

# =====================
# MAIN EXECUTION
# =====================
def main():
    selected_mode = select_trading_mode()
    engine = BacktestEngine(mode=selected_mode)
    ib = IB()
    ib.connect('127.0.0.1', 7497, clientId=1)
    initial_balance = engine.balance  # Get initial balance from engine

    start_date = None
    end_date = None
    try:
        # Backtest mode
        daily_data = get_historical_data(ib, symbol, backtest=True)
        run_backtest(daily_data)
        plot_candlestick(daily_data, backtest=True)

    except Exception as e:
        logger.error(f"Execution error: {str(e)}")
    finally:
        if 'ib' in locals() and ib.isConnected():
            ib.disconnect()

            if engine.trade_history:
                trade_history = pd.DataFrame(engine.trade_history)
                if 'timestamp' in trade_history.columns and len(trade_history) > 1:
                    start_dt = pd.to_datetime(trade_history['timestamp'].min())
                    end_dt = pd.to_datetime(trade_history['timestamp'].max())
                    days = (end_dt - start_dt).days + 1
                    if days < 1: days = 1  # Prevent division by zero
                    annualized_return = ((engine.balance/initial_balance) ** (252/days) - 1) * 100
                else:
                    annualized_return = 0.0  # No trades or insufficient data
                    
                if 'pnl' in trade_history.columns:
                    # Use only completed trades (SELL actions) for metrics
                    completed_trades = trade_history[trade_history['action'] == 'SELL']
                    total_trades = len(completed_trades)
                    profitable_trades = (completed_trades['pnl'] > 0).sum()
                    losing_trades = (completed_trades['pnl'] < 0).sum()
                    win_rate = (profitable_trades / total_trades * 100) if total_trades > 0 else 0.0

                    # Calculate returns using ONLY completed trades
                    returns = completed_trades['pnl'] / initial_balance
                    total_pnl = completed_trades['pnl'].sum()
                    annualized_return = ((engine.balance/initial_balance) ** (252/days) - 1) * 100

                    # Time-based annualization (fixed)
                    days = 1  # Default to avoid division by zero
                    if 'timestamp' in trade_history.columns and len(trade_history) > 1:
                        start_dt = pd.to_datetime(trade_history['timestamp'].min())
                        end_dt = pd.to_datetime(trade_history['timestamp'].max())
                        days = (end_dt - start_dt).days + 1
                        annualized_return = ((engine.balance/initial_balance) ** (252/days) - 1) * 100
                    else:
                        annualized_return = 0.0

                    # Risk metrics
                    risk_free_rate = 0.02  # 2% annual
                    excess_returns = returns - (risk_free_rate/252)
                    volatility = returns.std() * np.sqrt(252) if len(returns) > 1 else 0.0

                    # Sharpe Ratio (handle zero volatility)
                    sharpe_ratio = (np.sqrt(252) * excess_returns.mean() / returns.std() 
                                   if returns.std() > 0 else 0.0)

                    # Sortino Ratio (handle no downside risk)
                    downside_returns = returns[returns < 0]
                    if len(downside_returns) > 0:
                        sortino_ratio = np.sqrt(252) * excess_returns.mean() / downside_returns.std()
                    else:
                        sortino_ratio = float('inf') if excess_returns.mean() > 0 else 0.0

                    # Drawdown calculation
                    cumulative_returns = (1 + returns).cumprod()
                    rolling_max = cumulative_returns.expanding().max()
                    drawdowns = (cumulative_returns - rolling_max) / rolling_max
                    max_drawdown = drawdowns.min() * 100 if len(drawdowns) > 0 else 0.0

                    # Trade statistics
                    avg_profit = completed_trades[completed_trades['pnl'] > 0]['pnl'].mean()
                    avg_loss = abs(completed_trades[completed_trades['pnl'] < 0]['pnl'].mean())

                    # Profit factor with safeguards
                    gains = completed_trades[completed_trades['pnl'] > 0]['pnl']
                    losses = completed_trades[completed_trades['pnl'] < 0]['pnl']
                    total_gains = gains.sum()
                    total_losses = -losses.sum()

                    if total_losses == 0:
                        profit_factor = float('inf') if total_gains > 0 else 0.0
                    else:
                        profit_factor = total_gains / total_losses

                    # Log metrics
                    logger.info(f"Total Trades: {total_trades}")
                    logger.info(f"Profitable Trades: {profitable_trades}")
                    logger.info(f"Losing Trades: {losing_trades}")
                    logger.info(f"Win Rate: {win_rate:.2f}%")
                    logger.info(f"Total PnL: ${total_pnl:.2f}")
                    logger.info(f"Annualized Return: {annualized_return:.2f}%")
                    logger.info(f"Volatility: {volatility:.2f}")
                    logger.info(f"Sharpe Ratio: {sharpe_ratio:.2f}")
                    logger.info(f"Sortino Ratio: {sortino_ratio:.2f}" 
                              if not np.isinf(sortino_ratio) else "Sortino Ratio: Infinite")
                    logger.info(f"Max Drawdown: {max_drawdown:.2f}%")
                    logger.info(f"Average Profit: ${avg_profit:.2f}" 
                              if not np.isnan(avg_profit) else "Average Profit: $0.00")
                    logger.info(f"Average Loss: ${-avg_loss:.2f}" 
                              if not np.isnan(avg_loss) else "Average Loss: $0.00")
                    logger.info(f"Profit Factor: Infinite" 
                              if profit_factor == float('inf') else f"Profit Factor: {profit_factor:.2f}")
                    
                    pd.DataFrame(engine.trade_history).to_csv('trade_debug.csv')

                    if 'timestamp' in trade_history.columns and not trade_history['timestamp'].empty:
                        # Convert to UTC timezone-aware datetime
                        trade_history['timestamp'] = pd.to_datetime(trade_history['timestamp'], utc=True)
                        
                        # Get timezone-aware min/max directly from the series
                        start_date = trade_history['timestamp'].min().replace(
                            hour=0, minute=0, second=0, microsecond=0
                        )
                        end_date = trade_history['timestamp'].max().replace(
                            hour=23, minute=59, second=59, microsecond=999
                        )
                        
                        '''plot_pnl_calendar(engine, start_date, end_date, initial_balance, symbol)'''
                    else:
                        logger.warning("No trade history or missing timestamp data. Skipping PNL calendar plot.")

if __name__ == '__main__':
    main()
