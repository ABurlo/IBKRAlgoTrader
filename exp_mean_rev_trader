# =====================
# CORE IMPORTS
# =====================
from ib_insync import *
import pandas as pd
import numpy as np
import mplfinance as mpf
from datetime import datetime, timedelta, time
import asyncio
import nest_asyncio
import logging
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from time import sleep
from matplotlib.colors import ListedColormap, Normalize, LinearSegmentedColormap
from collections.abc import MutableMapping
import pytz
import re
import matplotlib.patheffects as pe
from matplotlib.cm import ScalarMappable
import seaborn as sns
from matplotlib.dates import DateFormatter
from scipy.signal import savgol_filter

# =====================
# LOGGING SETUP
# =====================
class TradeFormatter(logging.Formatter):
    GREEN = '\033[92m'
    RED = '\033[91m'
    WHITE = '\033[0m'
    
    def format(self, record):
        message = super().format(record)
        message = self.colorize_actions(message)
        message = self.colorize_values(message)
        return message

    def colorize_actions(self, message):
        return re.sub(r'^(BUY|SELL)(\s+\|)', 
                      lambda m: f"{self.GREEN if m.group(1) == 'BUY' else self.RED}{m.group(1)}{self.WHITE}{m.group(2)}",
                      message, count=1)
        
    def colorize_values(self, message):
        def color_repl(match):
            value_str = match.group(1) or match.group(2)
            try:
                value = float(value_str)
                color = self.GREEN if value > 0 else self.RED
            except:
                color = self.WHITE
            return f"{color}{match.group(0)}{self.WHITE}"
        
        parts = message.split(' | ')
        for i, part in enumerate(parts):
            if part.startswith(('Price:', 'Size:')):
                continue
            parts[i] = re.sub(r'\$ *(-?\d+\.\d+)|(-?\d+\.\d+)%', color_repl, part)
        return ' | '.join(parts)

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)

console_handler = logging.StreamHandler()
console_handler.setFormatter(TradeFormatter('%(asctime)s | %(message)s'))
file_handler = logging.FileHandler('strategy_execution.log')
file_handler.setFormatter(logging.Formatter('%(asctime)s | %(message)s'))
root_logger.addHandler(console_handler)
root_logger.addHandler(file_handler)
logger = logging.getLogger(__name__)

# =====================
# GLOBAL CONFIGURATION
# =====================
class GlobalConfig:
    nest_asyncio.apply()
    symbol = 'TSLA'
    durationStr = '1 D'
    barSizeSetting = '5 mins'
    currency = 'USD'
    initial_balance = 10000
    MIN_REQUIRED_BANDS = 2
    gradient_levels = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
    colors = ['#FF0000', '#FFA500', '#00FF00', '#0000FF']
    outer_levels = ['weak', 'moderate', 'strong']

sns.set_style("whitegrid")
sns.set_context("notebook")
sns.set_palette("tab10")
plt.rcParams.update({
    'font.size': 10,
    'axes.labelcolor': 'black',
    'axes.titlepad': 12,
    'grid.color': 'gray',
    'figure.dpi': 300,
    'axes.facecolor': 'white',
    'figure.facecolor': 'white'
})

MRC_GRADIENT_STYLE = 'full_spectrum'
full_spectrum_colors = ['#FF0000', '#FFA500', '#00FF00', '#0000FF']
def create_colormap(colors_list):
    return LinearSegmentedColormap.from_list('custom_mrc', colors_list, N=len(GlobalConfig.gradient_levels))
full_spectrum_cmap = create_colormap(full_spectrum_colors)
mrc_cmap = full_spectrum_cmap

def create_colormap(colors_list):
    return LinearSegmentedColormap.from_list('custom_mrc', colors_list, N=len(GlobalConfig.gradient_levels))

# =====================
# LINKED LIST DICTIONARY IMPLEMENTATION
# =====================
class LLNode:
    __slots__ = ('key', 'value', 'next')
    def __init__(self, key, value, next=None):
        self.key = key
        self.value = value
        self.next = next

class LLDict(MutableMapping):
    def __init__(self):
        self.head = None
        self._size = 0

    def __setitem__(self, key, value):
        current = self.head
        while current is not None:
            if current.key == key:
                current.value = value
                return
            current = current.next
        self.head = LLNode(key, value, self.head)
        self._size += 1

    def __getitem__(self, key):
        current = self.head
        while current is not None:
            if current.key == key:
                return current.value
            current = current.next
        raise KeyError(key)

    def __delitem__(self, key):
        prev = None
        current = self.head
        while current is not None:
            if current.key == key:
                if prev:
                    prev.next = current.next
                else:
                    self.head = current.next
                self._size -= 1
                return
            prev = current
            current = current.next
        raise KeyError(key)

    def __iter__(self):
        current = self.head
        while current is not None:
            yield current.key
            current = current.next

    def __len__(self):
        return self._size

# =====================
# MARKET CLOSURE WARNING
# =====================
def enforce_ny_timezone(timestamp):
    ny_tz = pytz.timezone('America/New_York')
    if not isinstance(timestamp, pd.Timestamp):
        timestamp = pd.to_datetime(timestamp)
    if timestamp.tzinfo is None:
        logger.debug(f"Localizing naive timestamp {timestamp} to NY timezone")
        return ny_tz.localize(timestamp)
    else:
        logger.debug(f"Converting tz-aware timestamp {timestamp} to NY timezone")
        return timestamp.tz_convert(ny_tz)

def is_market_close(index):
    ny_tz = pytz.timezone('America/New_York')
    if index.tzinfo is None:
        logger.debug(f"Localizing naive index {index} to NY timezone")
        index = index.tz_localize(ny_tz)
    else:
        logger.debug(f"Converting tz-aware index {index} to NY timezone")
        index = index.tz_convert(ny_tz)
    
    current_time = index.time
    market_close_time = time(16, 0)
    time_delta = timedelta(minutes=10)
    
    dummy_date = index.date()
    market_close_dt = ny_tz.localize(datetime.combine(dummy_date, market_close_time))
    
    return (market_close_dt - time_delta) <= index <= market_close_dt

# =====================
# METRICS CALCULATION
# =====================
def calculate_day_metrics(trades, risk_free_rate=0.02):
    if not trades:
        return {
            'overall': {
                'win_rate': 0.0,
                'win_loss_ratio': 0.0,
                'num_trades': 0,
                'total_pnl': 0.0,
                'avg_pnl': 0.0,
                'sharpe_ratio': 0.0,
                'sortino_ratio': 0.0,
                'max_drawdown': 0.0,
                'avg_win': 0.0,
                'avg_loss': 0.0,
                'profit_factor': 0.0,
                'avg_holding_hours': 0.0,
                'trades_per_day': 0.0,
                'risk_reward_ratio': 0.0,
                'recovery_factor': 0.0
            },
            'by_exit_strategy': {},
            'holding_periods': {
                'winning': 0.0,
                'losing': 0.0,
                'by_exit_strategy': {}
            },
            'volatility': {
                'avg_atr': 0.0,
                'pnl_volatility_correlation': 0.0
            }
        }

    num_trades = len(trades)
    total_pnl = 0.0
    wins = losses = 0
    pnl_list = []
    win_pnls = []
    loss_pnls = []
    holding_times = []
    exit_strategy_counts = {}
    exit_strategy_pnl = {}
    exit_strategy_wins = {}
    exit_strategy_losses = {}
    exit_strategy_holding = {}
    atr_list = []
    risk_rewards = []

    for t in trades:
        if not isinstance(t, dict) or 'timestamp' not in t or 'pnl' not in t:
            logger.error("Invalid trade format in calculate_day_metrics")
            continue
        
        trade_pnl = float(t.get('pnl', 0.0))
        total_pnl += trade_pnl
        pnl_list.append(trade_pnl)
        
        if trade_pnl > 0:
            wins += 1
            win_pnls.append(trade_pnl)
        elif trade_pnl < 0:
            losses += 1
            loss_pnls.append(trade_pnl)

        # Holding period
        entry_time = pd.to_datetime(t.get('entry_time', t['timestamp']))
        exit_time = pd.to_datetime(t['timestamp'])
        holding_time = (exit_time - entry_time).total_seconds() / 3600
        holding_times.append(holding_time)

        # Exit strategy breakdown
        exit_type = t.get('exit_type', 'Unknown')
        exit_strategy_counts[exit_type] = exit_strategy_counts.get(exit_type, 0) + 1
        exit_strategy_pnl[exit_type] = exit_strategy_pnl.get(exit_type, 0) + trade_pnl
        if trade_pnl > 0:
            exit_strategy_wins[exit_type] = exit_strategy_wins.get(exit_type, 0) + 1
        elif trade_pnl < 0:
            exit_strategy_losses[exit_type] = exit_strategy_losses.get(exit_type, 0) + 1
        exit_strategy_holding[exit_type] = exit_strategy_holding.get(exit_type, []) + [holding_time]

        # Volatility (ATR) and risk-reward
        if 'ATR' in t and 'entry_price' in t and 'price' in t:
            atr_list.append(float(t['ATR']))
            entry_price = float(t['entry_price'])
            exit_price = float(t['price'])
            potential_loss = entry_price - (entry_price - t['ATR'])
            potential_gain = abs(exit_price - entry_price) if trade_pnl > 0 else 0
            if potential_loss > 0 and potential_gain > 0:
                risk_rewards.append(potential_gain / potential_loss)

    # Overall Metrics
    win_rate = (wins / num_trades * 100) if num_trades > 0 else 0.0
    win_loss_ratio = (wins / losses) if losses > 0 else float('inf') if wins > 0 else 0.0
    avg_pnl = total_pnl / num_trades if num_trades > 0 else 0.0
    avg_win = sum(win_pnls) / len(win_pnls) if win_pnls else 0.0
    avg_loss = sum(loss_pnls) / len(loss_pnls) if loss_pnls else 0.0
    profit_factor = sum(win_pnls) / abs(sum(loss_pnls)) if loss_pnls else float('inf') if win_pnls else 0.0

    pnl_array = np.array(pnl_list)
    daily_rfr = risk_free_rate / 252
    excess_returns = pnl_array - daily_rfr
    mean_excess_return = np.mean(excess_returns)
    std_dev = np.std(pnl_array, ddof=1) if num_trades > 1 else 0.0
    sharpe_ratio = (mean_excess_return / std_dev) * np.sqrt(252) if std_dev > 0 else 0.0

    downside_returns = pnl_array[pnl_array < 0]
    downside_dev = np.std(downside_returns, ddof=1) if len(downside_returns) > 1 else 0.0
    sortino_ratio = (mean_excess_return / downside_dev) * np.sqrt(252) if downside_dev > 0 else 0.0

    if pnl_array.size > 0:
        cumulative_pnl = np.cumsum(pnl_array)
        peak = np.maximum.accumulate(cumulative_pnl)
        drawdowns = (peak - cumulative_pnl) / (peak + 1e-8)
        max_drawdown = np.max(drawdowns) * 100
    else:
        max_drawdown = 0.0

    avg_holding_hours = sum(holding_times) / len(holding_times) if holding_times else 0.0
    days_span = (max(pd.to_datetime([t['timestamp'] for t in trades])) - 
                 min(pd.to_datetime([t['timestamp'] for t in trades]))).days + 1
    trades_per_day = num_trades / days_span if days_span > 0 else 0.0
    risk_reward_ratio = sum(risk_rewards) / len(risk_rewards) if risk_rewards else 0.0
    recovery_factor = total_pnl / (max_drawdown / 100 * GlobalConfig.initial_balance) if max_drawdown > 0 else float('inf')

    by_exit_strategy = {}
    holding_by_exit = {}
    for exit_type in exit_strategy_counts:
        num_trades_exit = exit_strategy_counts[exit_type]
        wins_exit = exit_strategy_wins.get(exit_type, 0)
        losses_exit = exit_strategy_losses.get(exit_type, 0)
        total_pnl_exit = exit_strategy_pnl[exit_type]
        
        by_exit_strategy[exit_type] = {
            'win_rate': (wins_exit / num_trades_exit * 100) if num_trades_exit > 0 else 0.0,
            'num_trades': num_trades_exit,
            'total_pnl': round(total_pnl_exit, 2),
            'avg_pnl': total_pnl_exit / num_trades_exit if num_trades_exit > 0 else 0.0,
            'win_loss_ratio': (wins_exit / losses_exit) if losses_exit > 0 else float('inf') if wins_exit > 0 else 0.0
        }
        holding_by_exit[exit_type] = sum(exit_strategy_holding[exit_type]) / len(exit_strategy_holding[exit_type]) if exit_strategy_holding[exit_type] else 0.0

    winning_holding = [ht for ht, pnl in zip(holding_times, pnl_list) if pnl > 0]
    losing_holding = [ht for ht, pnl in zip(holding_times, pnl_list) if pnl < 0]
    avg_winning_holding = sum(winning_holding) / len(winning_holding) if winning_holding else 0.0
    avg_losing_holding = sum(losing_holding) / len(losing_holding) if losing_holding else 0.0

    avg_atr = sum(atr_list) / len(atr_list) if atr_list else 0.0
    volatility_pnl_correlation = np.corrcoef(pnl_list, atr_list[:len(pnl_list)])[0, 1] if atr_list and len(pnl_list) <= len(atr_list) else 0.0

    return {
        'overall': {
            'win_rate': round(win_rate, 2),
            'win_loss_ratio': round(win_loss_ratio, 2) if win_loss_ratio != float('inf') else float('inf'),
            'num_trades': num_trades,
            'total_pnl': round(total_pnl, 2),
            'avg_pnl': round(avg_pnl, 2),
            'sharpe_ratio': round(sharpe_ratio, 2),
            'sortino_ratio': round(sortino_ratio, 2),
            'max_drawdown': round(max_drawdown, 2),
            'avg_win': round(avg_win, 2),
            'avg_loss': round(avg_loss, 2),
            'profit_factor': round(profit_factor, 2) if profit_factor != float('inf') else float('inf'),
            'avg_holding_hours': round(avg_holding_hours, 2),
            'trades_per_day': round(trades_per_day, 2),
            'risk_reward_ratio': round(risk_reward_ratio, 2),
            'recovery_factor': round(recovery_factor, 2) if recovery_factor != float('inf') else float('inf')
        },
        'by_exit_strategy': by_exit_strategy,
        'holding_periods': {
            'winning': round(avg_winning_holding, 2),
            'losing': round(avg_losing_holding, 2),
            'by_exit_strategy': {k: round(v, 2) for k, v in holding_by_exit.items()}
        },
        'volatility': {
            'avg_atr': round(avg_atr, 2),
            'pnl_volatility_correlation': round(volatility_pnl_correlation, 2)
        }
    }


# =====================
# METRICS CALCULATION (Unchanged)
# =====================
class BacktestEngine:
    def __init__(self, mode='time_based'):
        self.mode = mode
        self.mode_config = {
            'time_based': {
                'position_risk': 0.02,  # 2% of balance per trade
                'volatility_mult': 1.5,  # Multiplier for volatility-based sizing
                'entry_conditions': {
                    'mean_deviation': {'threshold': -1.5, 'operator': '<'},  # Price below meanline by 1.5x meanrange
                    'rsi': {'threshold': 35, 'operator': '<'},  # RSI < 35 (slightly relaxed from 30)
                    'volatility_filter': {'threshold': 0.05, 'operator': '<'}  # ATR/close < 5% to avoid high volatility
                },
                'exit_conditions': {
                    'mean_reversion': {'threshold': 0.5, 'operator': '>'},  # Exit when price reverts to within 0.5x meanrange
                    'stop_loss': {'threshold': -2.5, 'operator': '<'},  # Stop loss if price drops 2.5x meanrange below entry
                    'time_limit': {'hours': 24}  # Max holding time of 24 hours
                },
                'signal_weights': {
                    'rsi': 0.4,  # Increased weight for RSI
                    'mean_deviation': 0.4,  # New weight for mean deviation
                    'volatility': 0.2  # Weight for volatility filter
                }
            },
            'signal_score': {
                'position_risk': 0.02,
                'volatility_mult': 1.5,
                'entry_conditions': {
                    'sma_crossover': {'short_window': 10, 'long_window': 50, 'direction': 'up'},
                    'rsi': {'threshold': 30, 'operator': '<'},
                    'williams_r': {'threshold': -85, 'operator': '<'},
                    'vwap': {'threshold': 'close < VWAP_shifted', 'operator': 'custom'}
                },
                'exit_conditions': {
                    'signal_score': {'threshold': 0.6}
                },
                'signal_weights': {
                    'rsi': 0.25, 'williams': 0.25, 'macd': 0.2, 'vwap': 0.2, 'adx': 0.05, 'volume': 0.05
                }
            },
            'volatility_adjusted': {
                'position_risk': 0.02,
                'volatility_mult': 1.5,
                'entry_conditions': {
                    'mean_deviation': {'threshold': -1.5, 'operator': '<'},
                    'rsi': {'threshold': 35, 'operator': '<'},
                    'adx': {'threshold': 20, 'operator': '<'}
                },
                'exit_conditions': {
                    'mean_deviation': {'threshold': 0.5, 'operator': '>'}
                },
                'signal_weights': {
                    'rsi': 0.2, 'williams': 0.2, 'macd': 0.15, 'vwap': 0.25, 'adx': 0.15, 'volume': 0.05
                }
            }
        }
        self.validate_mode_config()
        self.balance = GlobalConfig.initial_balance
        self.positions = LLDict()
        self.trade_history = []
        self.current_position = None
        self.fee_per_share = 0.0035
        self.slippage = 0.0002
        self.volatility_filter = {'min_mrc': 0.02, 'max_mrc': 0.05}
        self.daily_pnl = {}
        self.backtest_data = []
        logger.info(f"BacktestEngine initialized in {mode} mode with initial balance: ${self.balance}")

    @property
    def max_position_size(self):
        return self.balance * 0.25

    def validate_mode_config(self):
        required_keys = ['position_risk', 'volatility_mult', 'entry_conditions', 'exit_conditions', 'signal_weights']
        for mode, config in self.mode_config.items():
            if not all(k in config for k in required_keys):
                raise ValueError(f"Missing config keys in {mode}")
        logger.debug("Mode configuration validated successfully")

    def calculate_position_size(self, row):
        config = self.mode_config[self.mode]
        risk_capital = self.balance * config['position_risk']
        combined_vol = (max(0.01, row['meanrange']) * config['volatility_mult'] + row['ATR']) / 2
        position_size = min(self.max_position_size, risk_capital / combined_vol)
        logger.debug(f"Position size calculated: ${position_size:.2f} (Risk Capital=${risk_capital:.2f}, Combined Vol={combined_vol:.2f})")
        return position_size

    def composite_signal_score(self, row):
        config = self.mode_config[self.mode]
        weights = config['signal_weights']
        total_weight = sum(weights.values())
        scores = {}
        conditions_met = 0
        total_conditions = len(weights) + 2  # +2 for +DI/-DI and Stochastic

        volatility_regime = 1 + min(2, (row['ATR'] / row['close']) * 2.5) if not pd.isna(row['ATR']) else 1.0
        logger.debug(f"Volatility regime at {row.name}: {volatility_regime:.2f}")

        if 'rsi' in weights:
            rsi_value = row['RSI_shifted']
            scores['rsi'] = weights['rsi'] if rsi_value > 70 else 0
            conditions_met += 1 if rsi_value > 70 else 0
        if 'williams' in weights:
            williams_value = row['Williams_%R_shifted']
            scores['williams'] = weights['williams'] if williams_value > -20 else 0
            conditions_met += 1 if williams_value > -20 else 0
        if 'macd' in weights:
            scores['macd'] = weights['macd'] if row['MACD_shifted'] > row['Signal_shifted'] else 0
            conditions_met += 1 if row['MACD_shifted'] > row['Signal_shifted'] else 0
        if 'vwap' in weights:
            scores['vwap'] = weights['vwap'] if row['VWAP_shifted'] < row['close_shifted'] else 0
            conditions_met += 1 if row['VWAP_shifted'] < row['close_shifted'] else 0
        if 'adx' in weights:
            scores['adx'] = weights['adx'] if row['adx_shifted'] > 25 else 0
            conditions_met += 1 if row['adx_shifted'] > 25 else 0
        if 'volume' in weights:
            scores['volume'] = weights['volume'] if row['volume'] > row['volume_ma'] else 0
            conditions_met += 1 if row['volume'] > row['volume_ma'] else 0

        conditions_met += 1 if row['+di_shifted'] > row['-di_shifted'] else 0
        conditions_met += 1 if row['%K_shifted'] > row['%D_shifted'] and row['%D_shifted'] > 80 else 0

        weighted_score = sum(scores.values()) / total_weight if total_weight > 0 else 0
        percentage_score = (conditions_met / total_conditions) * volatility_regime * weighted_score
        return min(1.0, percentage_score)

    def apply_slippage(self, price, is_buy):
        slippage_factor = 1 + (self.slippage if is_buy else -self.slippage)
        return price * slippage_factor
    
    def calculate_fees(self, shares):
        return abs(shares * self.fee_per_share)

    def execute_trade(self, price, shares, action, timestamp, exit_type=None, row=None):
        try:
            price_with_slippage = self.apply_slippage(price, action == 'BUY')
            fees = self.calculate_fees(shares)
            position_value = shares * price_with_slippage

            trade = {
                'timestamp': timestamp,
                'action': action,
                'price': price_with_slippage,
                'shares': shares,
                'position_value': position_value,
                'balance_before': self.balance,
                'exit_type': exit_type if action == 'SELL' else None
            }

            if action == 'BUY':
                if position_value > self.balance:
                    logger.warning(f"Insufficient funds for {GlobalConfig.symbol}: ${position_value:.2f} > ${self.balance:.2f}")
                    return None
                self.current_position = {
                    'symbol': GlobalConfig.symbol, 
                    'entry_price': price_with_slippage, 
                    'shares': shares, 
                    'timestamp': timestamp, 
                    'position_value': position_value, 
                    'entry_time': timestamp
                }
                self.balance -= (position_value + fees)
                trade['pnl'] = 0.0
                trade['pnl_pct'] = 0.0
                logger.info(f"Buy executed: Price=${price_with_slippage:.2f}, Shares={shares:.2f}, Value=${position_value:.2f}")
            elif action == 'SELL' and self.current_position:
                entry_price = self.current_position['entry_price']
                entry_value = self.current_position['position_value']
                exit_value = position_value
                pnl = exit_value - entry_value - fees
                pnl_pct = (pnl / entry_value) * 100 if entry_value != 0 else 0

                trade['pnl'] = pnl
                trade['pnl_pct'] = pnl_pct
                self.balance += (exit_value - fees)
                self.daily_pnl[timestamp.strftime('%Y-%m-%d')] = (
                    self.daily_pnl.get(timestamp.strftime('%Y-%m-%d'), 0) + pnl
                )
                self.current_position = None
                logger.info(f"Sell executed: Price=${price_with_slippage:.2f}, Shares={shares:.2f}, PnL=${pnl:.2f}")

            trade['balance_after'] = self.balance
            if row is not None:
                trade.update(row.to_dict())
            self.trade_history.append(trade)
            self.backtest_data.append(trade)
            return trade
        except Exception as e:
            logger.error(f"Trade execution failed at {timestamp}: {str(e)}", exc_info=True)
            return None

# =====================
# TRADE EXECUTION CORE
# =====================
def execute_entry(price, timestamp, position_size, row):
    timestamp = enforce_ny_timezone(pd.to_datetime(timestamp))
    # Calculate shares and round to 2 decimal places, ensuring a minimum of 0.01
    shares = max(0.01, round(position_size / price, 2))
    if not np.isfinite(shares):
        logger.error(f"Invalid share calculation: {position_size}/{price}")
        return None
    trade = engine.execute_trade(price=price, shares=shares, action='BUY', timestamp=timestamp, row=row)
    if trade:
        log_trade(trade)
    else:
        logger.warning(f"Buy failed at {timestamp}: Trade returned None")
    return trade

def execute_exit(price, timestamp, exit_type, row):
    if not engine.current_position:
        logger.warning("Exit attempted with no open position")
        return None
    timestamp = enforce_ny_timezone(pd.to_datetime(timestamp))
    position = engine.current_position
    exit_shares = min(position['shares'], engine.max_position_size)
    trade = engine.execute_trade(price=price, shares=exit_shares, action='SELL', timestamp=timestamp, exit_type=exit_type, row=row)
    if trade:
        log_trade(trade)
    return trade

# Initialize engine globally
engine = BacktestEngine()

# =====================
# CONDITION CHECKS
# =====================
def check_entry_conditions(df, index):
    if str(index.tz) != 'America/New_York':
        raise ValueError(f"Invalid timezone in entry check: {index.tz}")
    
    try:
        df_up_to_index = df.loc[:index]
        if df_up_to_index.empty:
            return False
        
        row = df.loc[index]
        config = engine.mode_config[engine.mode]
        conditions = config['entry_conditions']
        total_conditions = len(conditions) + 2  # position_ok and time_ok
        conditions_met = 0
        log_details = [f"Entry Check at {index}"]

        logger.debug(f"Row data at {index}: {row.to_dict()}")

        for condition_name, params in conditions.items():
            condition_met = False
            if condition_name == 'mean_deviation':
                meanline = row['meanline']
                meanrange = row['meanrange_shifted']
                deviation = (row['close'] - meanline) / meanrange if meanrange > 0 else 0
                condition_met = deviation < params['threshold']
                log_details.append(f"Mean Deviation: Value={deviation:.2f}, Threshold={params['threshold']}, Met={condition_met}")
            elif condition_name == 'rsi':
                value = row['RSI_shifted']
                if pd.isna(value):
                    logger.warning(f"RSI_shifted is NaN at {index}")
                    condition_met = False
                else:
                    condition_met = value < params['threshold']
                log_details.append(f"RSI: Value={value:.2f}, Threshold={params['threshold']}, Met={condition_met}")
            elif condition_name == 'volatility_filter':
                volatility = row['ATR'] / row['close'] if row['close'] > 0 else float('inf')
                condition_met = volatility < params['threshold']
                log_details.append(f"Volatility: Value={volatility:.2f}, Threshold={params['threshold']}, Met={condition_met}")
            else:
                logger.warning(f"Unknown entry condition: {condition_name}")
                continue
            conditions_met += 1 if condition_met else 0

        position_ok = engine.balance > 1000 and not engine.current_position
        conditions_met += 1 if position_ok else 0
        log_details.append(f"Position OK: Balance={engine.balance:.2f}, Has Position={bool(engine.current_position)}, Met={position_ok}")

        time_ok = not is_market_close(index)
        conditions_met += 1 if time_ok else 0
        log_details.append(f"Time OK: Market Closed={not time_ok}, Met={time_ok}")

        fraction_met = conditions_met / total_conditions
        position_size = engine.calculate_position_size(row) if conditions_met > 0 else 0
        log_details.append(f"Fraction Met: {conditions_met}/{total_conditions} ({fraction_met:.2%})")
        log_details.append(f"Current Price: ${row['close']:.2f}, Position Size: ${position_size:.2f}")
        log_details.append(f"Signal Strength: RSI={row['RSI_shifted']:.2f}, ATR={row['ATR']:.2f}")

        logger.info(" | ".join(log_details))

        entry_triggered = fraction_met >= 0.75  # Stricter threshold (was 0.6)
        if entry_triggered:
            logger.info(f"Entry conditions met at {index} with fraction {fraction_met:.2%}, triggering buy")
        else:
            logger.debug(f"Entry not triggered at {index}: Fraction met={fraction_met:.2%}")
        return entry_triggered
    
    except KeyError as e:
        logger.error(f"Missing data at {index}: '{e}'")
        return False
    except Exception as e:
        logger.error(f"Error in entry condition check at {index}: {str(e)}", exc_info=True)
        return False

def check_exit_conditions(df, index):
    if str(index.tz) != 'America/New_York':
        raise ValueError(f"Invalid timezone in exit check: {index.tz}")
    
    if not engine.current_position:
        return None
    
    try:
        row = df.loc[index]
        config = engine.mode_config[engine.mode]
        conditions = config['exit_conditions']
        current_time = enforce_ny_timezone(index)
        entry_time = engine.current_position['entry_time']
        entry_price = engine.current_position['entry_price']
        current_price = row['close']
        meanrange = row['meanrange_shifted']

        for condition_name, params in conditions.items():
            if condition_name == 'mean_reversion':
                deviation = (current_price - row['meanline']) / meanrange if meanrange > 0 else 0
                if deviation > params['threshold']:
                    logger.debug(f"Mean Reversion exit at {index}: Deviation={deviation:.2f} > {params['threshold']}")
                    return 'Mean Reversion'
            elif condition_name == 'stop_loss':
                deviation = (current_price - entry_price) / meanrange if meanrange > 0 else 0
                if deviation < params['threshold']:
                    logger.debug(f"Stop Loss exit at {index}: Deviation={deviation:.2f} < {params['threshold']}")
                    return 'Stop Loss'
            elif condition_name == 'time_limit':
                time_diff = (current_time - entry_time).total_seconds() / 3600
                if time_diff >= params['hours']:
                    logger.debug(f"Time Limit exit at {index}: Holding={time_diff:.2f}h >= {params['hours']}h")
                    return f'Time Limit ({params["hours"]}h)'
            else:
                logger.warning(f"Unknown exit condition: {condition_name}")
        
        return None
    
    except KeyError as e:
        logger.error(f"Missing data at {index}: '{e}'")
        return None
    except Exception as e:
        logger.error(f"Error in exit condition check at {index}: {str(e)}", exc_info=True)
        return None

def log_trade(trade):
    try:
        timestamp = pd.to_datetime(trade['timestamp'])
        if timestamp.tz is None:
            timestamp = timestamp.tz_localize('UTC')
        trade_time = timestamp.tz_convert('America/New_York')
        
        log_parts = [
            f"{TradeFormatter.GREEN if trade['action'] == 'BUY' else TradeFormatter.RED}"
            f"{trade['action'].ljust(6)}\033[0m | {GlobalConfig.symbol.ljust(5)}",
            f"Time:  {trade_time.strftime('%Y-%m-%d %H:%M:%S%z')}",
            f"Price: \033[97m${trade['price']:7.2f}\033[0m",
            f"Size:  \033[97m${trade['position_value']:7.2f}\033[0m",
            f"Shares: \033[97m{trade['shares']:>6.2f}\033[0m"
        ]
        if trade['action'] == 'SELL':
            pnl_color = TradeFormatter.GREEN if trade['pnl'] > 0 else TradeFormatter.RED
            log_parts.extend([
                f"PnL:   {pnl_color}${trade['pnl']:7.2f}{TradeFormatter.WHITE}",
                f"Return: {pnl_color}{trade['pnl_pct']:6.2f}%{TradeFormatter.WHITE}",
                f"Exit:  {trade['exit_type']}"
            ])
        logger.info(" | ".join(log_parts))
    except Exception as e:
        logger.error(f"Failed to log trade: {str(e)}", exc_info=True)

def calculate_mrc_with_gradient(df, length=200, gradient_levels=GlobalConfig.gradient_levels, outer_levels=GlobalConfig.outer_levels):
    # Calculate HLC3 if not already present
    if 'hlc3' not in df.columns:
        df['hlc3'] = (df['high'] + df['low'] + df['close']) / 3

    # Calculate True Range (TR) for volatility
    df['prev_close'] = df['close'].shift(1).ffill()
    df['tr1'] = df['high'] - df['low']
    df['tr2'] = abs(df['high'] - df['prev_close'])
    df['tr3'] = abs(df['low'] - df['prev_close'])
    df['tr'] = df[['tr1', 'tr2', 'tr3']].max(axis=1)
    df.drop(['tr1', 'tr2', 'tr3', 'prev_close'], axis=1, inplace=True, errors='ignore')

    # Calculate supersmoother meanline with increased length and Savitzky-Golay smoothing
    df['meanline'] = supersmoother(df['hlc3'], length, apply_savgol=True).replace([np.inf, -np.inf, np.nan], np.nan).ffill().bfill()
    logger.debug(f"Meanline sample: {df['meanline'].head().tolist()}, NaN count: {df['meanline'].isna().sum()}")

    # Calculate smooth meanrange (use a rolling mean with additional smoothing)
    df['meanrange'] = df['tr'].rolling(20, min_periods=1).mean().replace([np.inf, -np.inf, np.nan], 0).ffill().bfill()
    df['meanrange'] = supersmoother(df['meanrange'], length=20, apply_savgol=True).replace([np.inf, -np.inf, np.nan], 0).ffill().bfill()
    logger.debug(f"Meanrange sample: {df['meanrange'].head().tolist()}, NaN count: {df['meanrange'].isna().sum()}")

    # Calculate smooth gradient bands
    for level in gradient_levels:
        df[f'upband_{level}'] = df['meanline'] + (df['meanrange'] * level)
        df[f'loband_{level}'] = df['meanline'] - (df['meanrange'] * level)
        df[f'upband_{level}_shifted'] = df[f'upband_{level}'].shift(1).ffill().bfill()
        df[f'loband_{level}_shifted'] = df[f'loband_{level}'].shift(1).ffill().bfill()
        logger.debug(f"Upband_{level} sample: {df[f'upband_{level}'].head().tolist()}, NaN count: {df[f'upband_{level}'].isna().sum()}")

    # Calculate outer levels with smooth data
    outer_multiples = {'weak': 3.0, 'moderate': 3.5, 'strong': 4.0}
    for level_name in outer_levels:
        multiple = outer_multiples[level_name]
        df[f'upband_{level_name}'] = df['meanline'] + (df['meanrange'] * multiple)
        df[f'loband_{level_name}'] = df['meanline'] - (df['meanrange'] * multiple)

    df['meanrange_shifted'] = df['meanrange'].shift(1).ffill().bfill()
    return df

def supersmoother(src: pd.Series, length: int = 200, apply_savgol: bool = True) -> pd.Series:
    if len(src) < 3 or length < 1:
        return src.ffill().bfill()
    
    # Calculate supersmoother
    a1 = np.exp(-np.sqrt(2) * np.pi / length)
    b1 = 2 * a1 * np.cos(np.sqrt(2) * np.pi / length)
    c3 = -a1**2
    c2 = b1
    c1 = 1 - c2 - c3
    
    ss = np.zeros_like(src, dtype=float)
    ss[0] = src.iloc[0] if not pd.isna(src.iloc[0]) else 0
    ss[1] = src.iloc[1] if not pd.isna(src.iloc[1]) else ss[0]
    for i in range(2, len(src)):
        ss[i] = c1 * (src.iloc[i] if not pd.isna(src.iloc[i]) else ss[i-1]) + c2 * ss[i-1] + c3 * ss[i-2]
    
    result = pd.Series(ss, index=src.index).replace([np.inf, -np.inf, np.nan], np.nan).ffill().bfill()
    
    # Apply Savitzky-Golay filter for additional smoothness (optional)
    if apply_savgol and len(result) >= 5:
        result = pd.Series(savgol_filter(result, window_length=min(31, len(result)), polyorder=3), index=result.index)
    
    logger.debug(f"Supersmoother output sample: {result.head().tolist()}, NaN count: {result.isna().sum()}")
    return result


def williams_r(high, low, close, lookback=14):
    highest_high = high.rolling(window=lookback, min_periods=1).max()
    lowest_low = low.rolling(window=lookback, min_periods=1).min()
    denominator = highest_high - lowest_low
    denominator = denominator.replace(0, np.nan)
    wr = -100 * ((highest_high - close) / denominator)
    return wr.fillna(-50).replace([np.inf, -np.inf], -50)

def stochastic_oscillator(df, k_period=14, d_period=3):
    low_min = df['low'].rolling(k_period).min()
    high_max = df['high'].rolling(k_period).max()
    df['%K'] = 100 * ((df['close'] - low_min) / (high_max - low_min))
    df['%K'] = df['%K'].fillna(50)
    df['%D'] = df['%K'].rolling(d_period).mean()
    return df

def get_historical_data(ib, exchange='SMART', currency='USD', backtest=False):
    """
    Retrieve and process historical data for backtesting from Interactive Brokers.
    
    Args:
        ib: IB instance from ib_insync for API connection.
        exchange: Stock exchange (default: 'SMART').
        currency: Currency for the stock (default: 'USD').
        backtest: Boolean flag to indicate backtest mode (default: False).
    
    Returns:
        DataFrame with processed historical data and technical indicators.
    """
    contract = Stock(GlobalConfig.symbol, exchange, currency)
    ib.qualifyContracts(contract)
    if backtest:
        logger.info(f"Requesting historical data for {GlobalConfig.symbol}")
        bars = ib.reqHistoricalData(
            contract,
            endDateTime='',
            durationStr=GlobalConfig.durationStr,  # e.g., '365 D'
            barSizeSetting=GlobalConfig.barSizeSetting,  # e.g., '1 day'
            whatToShow='TRADES',
            useRTH=True,
            formatDate=2,  # Use timezone-aware datetime
            keepUpToDate=False
        )
        
        if not bars:
            logger.error("No historical data received from IBKR")
            raise ValueError("No historical data received")
        
        df = util.df(bars)  # Convert bars to DataFrame
        if df.empty:
            logger.error("Empty DataFrame received from IB")
            raise ValueError("Empty DataFrame received from IB")

        # Set index and ensure timezone
        df['date'] = pd.to_datetime(df['date'], utc=True)
        df.set_index('date', inplace=True)
        df.index = df.index.tz_convert('America/New_York')

        # Validate and clean required columns
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in required_cols:
            if col not in df.columns:
                logger.error(f"Missing column {col}")
                raise ValueError(f"Missing column {col}")
            df[col] = pd.to_numeric(df[col], errors='coerce').replace([np.inf, -np.inf, np.nan], np.nan).ffill().bfill()
            if df[col].isna().all() or (df[col] <= 0).any():
                logger.error(f"Invalid data in {col}")
                raise ValueError(f"Invalid data in {col}")

        # Calculate HLC3 for VWAP and MRC
        df['hlc3'] = (df['high'] + df['low'] + df['close']) / 3

        # Calculate MRC (Mean Reversion Channel) with gradient bands
        df = calculate_mrc_with_gradient(df)

        # Calculate ADX and related indicators
        df = calculate_adx(df)

        # Calculate Stochastic Oscillator
        df = stochastic_oscillator(df)

        # VWAP with daily reset to avoid look-ahead across days
        df['VWAP'] = (df['hlc3'] * df['volume']).groupby(df.index.date).cumsum() / df['volume'].groupby(df.index.date).cumsum()

        # Calculate MACD
        exp12 = df['close'].ewm(span=12, adjust=False).mean()
        exp26 = df['close'].ewm(span=26, adjust=False).mean()
        df['MACD'] = exp12 - exp26
        df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
        df['Histogram'] = df['MACD'] - df['Signal']

        # Calculate RSI
        df['RSI'] = calculate_rsi(df['close'])
        logger.debug(f"RSI calculated - NaN count: {df['RSI'].isna().sum()}, Sample: {df['RSI'].head().tolist()}")

        # Calculate Williams %R
        df['Williams_%R'] = williams_r(df['high'], df['low'], df['close'])
        logger.debug(f"Williams_%R calculated - NaN count: {df['Williams_%R'].isna().sum()}")

        # Calculate ATR
        df['ATR'] = df['tr'].rolling(14).mean().replace([np.inf, -np.inf, np.nan], 0).ffill().bfill()
        logger.debug(f"ATR calculated - NaN count: {df['ATR'].isna().sum()}")

        # Calculate Volume MA
        df['volume_ma'] = df['volume'].rolling(20).mean().replace([np.inf, -np.inf, np.nan], 0).ffill().bfill()

        # Calculate SMA_20 (for reference, not used directly in conditions)
        df['SMA_20'] = df['close'].rolling(20).mean().replace([np.inf, -np.inf, np.nan], 0).ffill().bfill()

        # Shift all indicators to avoid look-ahead bias
        shifted_cols = ['close', 'RSI', 'Williams_%R', 'VWAP', 'MACD', 'Signal', 'adx', '+di', '-di', '%K', '%D', 'volume_ma', 'SMA_20', 'ATR']
        for col in shifted_cols:
            if col not in df.columns:
                logger.error(f"Cannot shift missing column: {col}")
                raise ValueError(f"Cannot shift missing column: {col}")
            df[f'{col}_shifted'] = df[col].shift(1).ffill().bfill()
            logger.debug(f"{col}_shifted - NaN count: {df[f'{col}_shifted'].isna().sum()}, Sample: {df[f'{col}_shifted'].head().tolist()}")

        # Clean up temporary columns
        df = df.drop(['vwap_numerator', 'vwap_denominator'], axis=1, errors='ignore')
        
        logger.info(f"Processed historical data with {len(df)} rows, Index range: {df.index.min()} to {df.index.max()}")
        return df

def calculate_macd(close, fast=12, slow=26, signal=9):
    ema_fast = close.ewm(span=fast, adjust=False).mean()
    ema_slow = close.ewm(span=slow, adjust=False).mean()
    macd = ema_fast - ema_slow
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    histogram = macd - signal_line
    return macd, signal_line, histogram

def calculate_williams_r(high, low, close, period=14):
    highest_high = high.rolling(window=period).max()
    lowest_low = low.rolling(window=period).min()
    return -100 * (highest_high - close) / (highest_high - lowest_low)

def calculate_adx(df, window=14):
    high = df['high']
    low = df['low']
    close = df['close']
    tr = np.maximum(high - low, np.maximum(np.abs(high - close.shift().bfill()), np.abs(low - close.shift().bfill())))
    up_move = high.diff()
    down_move = -low.diff()
    tr_smooth = tr.ewm(alpha=1/window, adjust=False).mean()
    plus_dm = up_move.where((up_move > down_move) & (up_move > 0), 0.0)
    minus_dm = down_move.where((down_move > up_move) & (down_move > 0), 0.0)
    plus_di = 100 * (plus_dm.ewm(alpha=1/window, adjust=False).mean() / tr_smooth)
    minus_di = 100 * (minus_dm.ewm(alpha=1/window, adjust=False).mean() / tr_smooth)
    di_diff = np.abs(plus_di - minus_di)
    di_sum = (plus_di + minus_di).replace(0, 1e-8)
    dx = 100 * (di_diff / di_sum)
    adx = dx.ewm(alpha=1/window, adjust=False).mean().clip(0, 100)
    df['adx'] = adx
    df['+di'] = plus_di
    df['-di'] = minus_di
    df['adx_shifted'] = adx.shift(1).fillna(0)
    return df

def calculate_rsi(close, period=14):
    delta = close.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

def calculate_plus_di(high, low, close, period=14):
    plus_dm = (high - high.shift()).where((high - high.shift()) > (low.shift() - low), 0)
    tr = pd.concat([high - low, (high - close.shift()).abs(), (low - close.shift()).abs()], axis=1).max(axis=1)
    atr = tr.rolling(window=period).mean()
    return 100 * plus_dm.rolling(window=period).mean() / atr

def calculate_minus_di(high, low, close, period=14):
    minus_dm = (low.shift() - low).where((low.shift() - low) > (high - high.shift()), 0)
    tr = pd.concat([high - low, (high - close.shift()).abs(), (low - close.shift()).abs()], axis=1).max(axis=1)
    atr = tr.rolling(window=period).mean()
    return 100 * minus_dm.rolling(window=period).mean() / atr

def calculate_stoch(high, low, close, k_period=14, d_period=3):
    lowest_low = low.rolling(window=k_period).min()
    highest_high = high.rolling(window=k_period).max()
    k = 100 * (close - lowest_low) / (highest_high - lowest_low)
    d = k.rolling(window=d_period).mean()
    return k, d

def calculate_vwap(high, low, close, volume):
    typical_price = (high + low + close) / 3
    vwap = (typical_price * volume).cumsum() / volume.cumsum()
    return vwap

def plot_candlestick(df, backtest=False, time_frame=GlobalConfig.barSizeSetting):
    if df.empty:
        logger.error("Cannot plot empty DataFrame")
        raise ValueError("Cannot plot empty DataFrame")

    required_cols = ['open', 'high', 'low', 'close', 'volume', 'meanline'] + \
                    [f'upband_{level}' for level in GlobalConfig.gradient_levels] + \
                    [f'loband_{level}' for level in GlobalConfig.gradient_levels]
    indicator_cols = ['MACD', 'Signal', 'Histogram', 'RSI', 'Williams_%R', 'adx', '%K', '%D', 'volume']
    for col in required_cols:
        if col not in df.columns:
            logger.error(f"Missing required column {col}")
            raise ValueError(f"Missing required column {col}")
        df[col] = pd.to_numeric(df[col], errors='coerce').replace([np.inf, -np.inf, np.nan], np.nan).ffill().bfill()
        if df[col].isna().all():
            logger.error(f"Column {col} contains only NaN values")
            raise ValueError(f"Column {col} contains only NaN values")
        logger.debug(f"{col} sample: {df[col].head().tolist()}, NaN count: {df[col].isna().sum()}")

    if df.index.tz != pytz.timezone('America/New_York'):
        df.index = df.index.tz_convert('America/New_York')
    df = df.sort_index()

    logger.debug(f"Index range: {df.index.min()} to {df.index.max()}")
    for col in indicator_cols:
        if col in df.columns:
            logger.debug(f"{col} NaN count: {df[col].isna().sum()}")

    style = mpf.make_mpf_style(
        base_mpf_style='classic',
        marketcolors=mpf.make_marketcolors(up='#00FF00', down='#FF0000', edge='black', wick='black', volume='gray'),
        gridstyle=':', gridcolor='gray', facecolor='white'
    )

    addplots = []
    panels_used = set()

    if 'volume' in df.columns:
        addplots.append(mpf.make_addplot(df['volume'], panel=1, type='bar', color='gray', ylabel='Volume'))
        panels_used.add(1)

    if all(col in df.columns for col in ['MACD', 'Signal', 'Histogram']):
        addplots.extend([
            mpf.make_addplot(df['MACD'], panel=2, color='#1f77b4', width=1.5, ylabel='MACD'),
            mpf.make_addplot(df['Signal'], panel=2, color='#ff7f0e', width=1.5),
            mpf.make_addplot(df['Histogram'], panel=2, type='bar', color=np.where(df['Histogram'] >= 0, '#5cb85c', '#d62728'), alpha=0.6)
        ])
        panels_used.add(2)

    if 'RSI' in df.columns:
        addplots.append(mpf.make_addplot(df['RSI'], panel=3, color='#4B0082', width=1.5, ylim=(0, 100), ylabel='RSI'))
        panels_used.add(3)

    if 'Williams_%R' in df.columns:
        addplots.append(mpf.make_addplot(df['Williams_%R'], panel=4, color='#9467bd', width=1.5, ylim=(-100, 0), ylabel='Williams %R'))
        panels_used.add(4)

    if 'adx' in df.columns:
        addplots.append(mpf.make_addplot(df['adx'], panel=5, color='#2ca02c', width=1.5, ylim=(0, 100), ylabel='ADX'))
        panels_used.add(5)

    if all(col in df.columns for col in ['%K', '%D']):
        addplots.extend([
            mpf.make_addplot(df['%K'], panel=6, color='#8c564b', width=1.5, ylabel='Stochastic'),
            mpf.make_addplot(df['%D'], panel=6, color='#e377c2', width=1.5)
        ])
        panels_used.add(6)

    num_panels = 1 + len(panels_used)
    panel_ratios = [10] + [3] * len(panels_used)

    fig, axlist = mpf.plot(
        df[['open', 'high', 'low', 'close']],
        type='candle',
        style=style,
        addplot=addplots,
        volume=False,
        panel_ratios=tuple(panel_ratios),
        figsize=(20, 8 + 2 * len(panels_used)),
        title=f'{GlobalConfig.symbol} Candlestick Chart - {GlobalConfig.barSizeSetting} (Backtest: {backtest})',
        returnfig=True,
        datetime_format='%Y-%m-%d %H:%M'
    )

    plot_mrc_gradient(axlist[0], df)

    for i, ax in enumerate(axlist):
        ax.set_facecolor('white')
        ax.grid(True, color='gray', linestyle=':', linewidth=0.5, alpha=0.5)
        ax.tick_params(colors='black', labelsize=8)
        ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))
        ax.set_xlim(df.index[0], df.index[-1])

    y_min = min(df['low'].min(), df[[f'loband_{level}' for level in GlobalConfig.gradient_levels]].min().min()) * 0.95
    y_max = max(df['high'].max(), df[[f'upband_{level}' for level in GlobalConfig.gradient_levels]].max().max()) * 1.05
    axlist[0].set_ylim(y_min, y_max)

    fig.set_facecolor('white')
    plt.tight_layout(pad=2.0)
    logger.info("Chart plotted successfully")
    plt.show()

def plot_mrc_gradient(ax, df, time_frame=GlobalConfig.barSizeSetting):
    # Calculate supersmoother meanline with increased length and Savitzky-Golay smoothing
    meanline = supersmoother(df['hlc3'], length=200, apply_savgol=True).replace([np.inf, -np.inf, np.nan, 0], np.nan).ffill().bfill()
    if meanline.empty or meanline.isna().all():
        logger.error("Meanline data is empty or contains only NaN values")
        return

    # Plot the supersmoother meanline with enhanced visibility, smoothness, and interpolation
    ax.plot(df.index, meanline, color='purple', linestyle='-', linewidth=2.5, zorder=15, label='Meanline (Supersmoother)', 
            path_effects=[pe.Stroke(linewidth=3.5, foreground='purple', alpha=0.3), pe.Normal()], 
            antialiased=True, drawstyle='steps-post')  # Using steps-post for smoother appearance

    # Normalize the gradient levels for coloring
    norm = Normalize(vmin=min(GlobalConfig.gradient_levels), vmax=max(GlobalConfig.gradient_levels))

    # Get price range for clipping
    price_min = df['low'].min() * 0.9
    price_max = df['high'].max() * 1.1

    # Plot the gradient bands with interpolation for smoothness, matching df.index length
    num_points = len(df.index)  # Use the length of df.index to match the DataFrame
    for i in range(len(GlobalConfig.gradient_levels)):
        level = GlobalConfig.gradient_levels[i]
        upper_band = df[f'upband_{level}'].replace([np.inf, -np.inf, np.nan, 0], np.nan).ffill().bfill()
        lower_band = df[f'loband_{level}'].replace([np.inf, -np.inf, np.nan, 0], np.nan).ffill().bfill()
        color = mrc_cmap(norm(level))

        # Clip bands to prevent extreme values
        upper_band = np.clip(upper_band, price_min, price_max)
        lower_band = np.clip(lower_band, price_min, price_max)

        # Interpolate to match the length of df.index for smoother appearance
        upper_band_interpolated = np.interp(np.linspace(0, len(upper_band)-1, num_points), np.arange(len(upper_band)), upper_band)
        lower_band_interpolated = np.interp(np.linspace(0, len(lower_band)-1, num_points), np.arange(len(lower_band)), lower_band)
        meanline_interpolated = np.interp(np.linspace(0, len(meanline)-1, num_points), np.arange(len(meanline)), meanline)

        if i == 0:
            ax.fill_between(df.index, meanline_interpolated, upper_band_interpolated, color=color, alpha=0.5, zorder=2 + i, label=f'Level {level}', 
                            interpolate=True)
            ax.fill_between(df.index, meanline_interpolated, lower_band_interpolated, color=color, alpha=0.5, zorder=2 + i, 
                            interpolate=True)
        else:
            prev_level = GlobalConfig.gradient_levels[i - 1]
            prev_upper_band = df[f'upband_{prev_level}'].replace([np.inf, -np.inf, np.nan, 0], np.nan).ffill().bfill()
            prev_lower_band = df[f'loband_{prev_level}'].replace([np.inf, -np.inf, np.nan, 0], np.nan).ffill().bfill()
            prev_upper_band = np.clip(prev_upper_band, price_min, price_max)
            prev_lower_band = np.clip(prev_lower_band, price_min, price_max)
            prev_upper_band_interpolated = np.interp(np.linspace(0, len(prev_upper_band)-1, num_points), np.arange(len(prev_upper_band)), prev_upper_band)
            prev_lower_band_interpolated = np.interp(np.linspace(0, len(prev_lower_band)-1, num_points), np.arange(len(prev_lower_band)), prev_lower_band)

            ax.fill_between(df.index, prev_upper_band_interpolated, upper_band_interpolated, color=color, alpha=0.5, zorder=2 + i, label=f'Level {level}', 
                            interpolate=True)
            ax.fill_between(df.index, prev_lower_band_interpolated, lower_band_interpolated, color=color, alpha=0.5, zorder=2 + i, 
                            interpolate=True)

    # Add legend and adjust plot limits
    ax.legend(loc='upper left', fontsize=8, bbox_to_anchor=(0, 1))
    y_min = min(df['low'].min(), lower_band.min()) * 0.95
    y_max = max(df['high'].max(), upper_band.max()) * 1.05
    ax.set_ylim(y_min, y_max)
    ax.set_xlim(df.index[0], df.index[-1])

def print_backtest_report(metrics):
    """Print a formatted report of the backtest metrics."""
    print("\n=== Backtest Performance Report ===")
    print("\nOverall Metrics:")
    for key, value in metrics['overall'].items():
        print(f"  {key.replace('_', ' ').title()}: {value if value != float('inf') else '∞'}")

    print("\nPerformance by Exit Strategy:")
    for strategy, stats in metrics['by_exit_strategy'].items():
        print(f"  {strategy}:")
        for key, value in stats.items():
            print(f"    {key.replace('_', ' ').title()}: {value if value != float('inf') else '∞'}")

    print("\nHolding Periods (hours):")
    print(f"  Winning Trades: {metrics['holding_periods']['winning']}")
    print(f"  Losing Trades: {metrics['holding_periods']['losing']}")
    print("  By Exit Strategy:")
    for strategy, hours in metrics['holding_periods']['by_exit_strategy'].items():
        print(f"    {strategy}: {hours}")

    print("\nVolatility Metrics:")
    print(f"  Average ATR: {metrics['volatility']['avg_atr']}")
    print(f"  PnL-Volatility Correlation: {metrics['volatility']['pnl_volatility_correlation']}")
    
# =====================
# MODE SELECTION
# =====================
def select_trading_mode():
    print("\nSelect Trading Strategy Mode:")
    print("1. Time-Based Exit Mean Reversion")
    print("2. Composite Signal Score Mean Reversion")
    print("3. Volatility-Adjusted Mean Reversion")
    choice = input("Enter choice (1-3): ").strip()
    modes = {'1': 'time_based', '2': 'signal_score', '3': 'volatility_adjusted'}
    selected_mode = modes.get(choice, 'time_based')
    logger.info(f"Selected trading mode: {selected_mode}")
    return selected_mode

# =====================
# RUN BACKTEST
# =====================
def run_backtest(df, mode):
    global engine
    engine = BacktestEngine(mode=mode)
    
    for index, row in df.iterrows():
        if not is_market_close(index):
            if check_entry_conditions(df, index):
                execute_entry(row['close'], index, engine.calculate_position_size(row), row)
            elif engine.current_position:
                exit_type = check_exit_conditions(df, index)
                if exit_type:
                    execute_exit(row['close'], index, exit_type, row)
            else:
                non_trade_data = {
                    'timestamp': index,
                    'action': 'NONE',
                    'price': row['close'],
                    'shares': 0,
                    'position_value': 0,
                    'pnl': 0.0,
                    'pnl_pct': 0.0,
                    'balance_before': engine.balance,
                    'balance_after': engine.balance,
                    'exit_type': None
                }
                non_trade_data.update(row.to_dict())
                engine.backtest_data.append(non_trade_data)

    logger.info(f"Backtest completed with {len(engine.trade_history)} trades")
    return engine, df

# =====================
# MAIN EXECUTION
# =====================
def main():
    global engine
    selected_mode = select_trading_mode()
    engine = BacktestEngine(mode=selected_mode)
    ib = IB()
    
    try:
        ib.connect('127.0.0.1', 7497, clientId=1)
        logger.info("Connected to IBKR")
        
        daily_data = get_historical_data(ib, backtest=True)
        if daily_data is None or daily_data.empty:
            raise ValueError("Failed to retrieve historical data")
        
        required_cols = ['open', 'high', 'low', 'close', 'volume', 'meanline'] + \
                        [f'upband_{level}' for level in GlobalConfig.gradient_levels] + \
                        [f'loband_{level}' for level in GlobalConfig.gradient_levels] + \
                        ['MACD', 'Signal', 'Histogram', 'RSI', 'Williams_%R', 'adx', '%K', '%D', 'VWAP', 'ATR']
        missing = [col for col in required_cols if col not in daily_data.columns]
        if missing:
            logger.error(f"Missing required columns: {missing}")
            raise ValueError(f"Missing required columns: {missing}")
        
        engine, daily_data = run_backtest(daily_data, selected_mode)
        
        logger.debug(f"Data shape: {daily_data.shape}")
        logger.debug(f"Columns available: {daily_data.columns.tolist()}")
        
        plot_candlestick(daily_data, backtest=True)

        metrics = calculate_day_metrics(engine.trade_history)
        print_backtest_report(metrics)

        backtest_df = pd.DataFrame(engine.backtest_data)
        backtest_df['timestamp'] = pd.to_datetime(backtest_df['timestamp']).dt.tz_convert('America/New_York')
        backtest_df.to_csv('backtest_raw_data.csv', index=False)
        logger.info("Backtest data exported to 'backtest_raw_data.csv'")
            
    except Exception as e:
        logger.error(f"Main execution error: {str(e)}", exc_info=True)
    finally:
        if ib.isConnected():
            ib.disconnect()
            logger.info("Disconnected from IBKR")

if __name__ == '__main__':
    main()
